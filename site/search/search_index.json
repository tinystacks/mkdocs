{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction It\u2019s not easy shipping software to the cloud. A team aiming to deploy its application backends to a cloud provider such as Amazon Web Services needs to understand a dozen or more AWS features just to stand up a small project. And as the scale and size of your project grows, so does the complexity of its deployment. Many companies soon find themselves spending more time making their cloud applications secure and scalable than they do writing its core functionality. TinyStacks takes the drudgery out of deployment. Using TinyStacks, development teams can: Create a multi-stage (dev/test/stage/prod) CI/CD pipeline. TinyStacks deploys directly to your own AWS account, making it easy to deploy onto the cloud quickly while also providing transparency and flexibility. Use your favorite languages and frameworks Deploy applications to production to the cloud with minimal overhead Scale applications and APIs from hundreds of users to millions with little additional effort By deploying with TinyStacks, your team can spend less time building out DevOps architecture and more time building out the core features that make your application unique! What TinyStacks Does Below are some of the benefits that applications gain by deploying with TinyStacks. Use Your Favorite Languages and Frameworks. TinyStacks is language- and framework-agnostic. Our deployment processes will work with any application deployed via a Docker container. If your application currently isn\u2019t Dockerized, we have pre-built template files for the Node.js, Django, Express, and Flask frameworks to get you started. Full-Stack Architecture. TinyStacks creates everything your application requires to run on the cloud. It can even provision a Postgres database for data storage. Multi-Stage Deployments. TinyStacks supports defining as many development stages as you need. By default, we create a dev stage for development purposes. Using the TinyStacks dashboard, you can easily add a production stage and any intermediate stages - such as testing and staging - to your deployment process. Continuous Integration and Continuous Deployment. A TinyStacks stack is wired to a repository you own in GitHub or GitLab. By default, any changes you make to a stack\u2019s corresponding branch will be automatically compiled and updated in the currently running environment. You can also define manual approvals for any stage. Security by Default. TinyStacks incorporates the latest security best practices for cloud-based applications. Cost Efficiency. Controlling costs on the cloud is difficult. Finding the best price point for your application can require hundreds of hours of testing and experimentation. TinyStacks constantly monitor, test, and change the way we deploy stacks for our customers to get the best performance at the lowest price. (Read about a major change we made to our deployments that saved up to 40% on stack costs for our customers.) What TinyStacks Doesn\u2019t Do While TinyStacks conveys many benefits to development teams, it can\u2019t do everything . The following areas remain your responsibility: Hosting. TinyStacks is a DevOps orchestration framework, not a cloud hosting provider. All provisioned cloud capacity is run in your own cloud provider\u2019s accounts and you are directly responsible for all costs incurred. End to End Security. TinyStacks cannot make an insecure application secure. While Tinystacks takes multiple measures to ensure your infrastructure is secure by default, it does not guarantee the security of your application. It is up to you and your team to design and implement applications that adhere to the tenets of good security, such as defensive programming and the principle of least privilege.","title":"Introduction"},{"location":"#introduction","text":"It\u2019s not easy shipping software to the cloud. A team aiming to deploy its application backends to a cloud provider such as Amazon Web Services needs to understand a dozen or more AWS features just to stand up a small project. And as the scale and size of your project grows, so does the complexity of its deployment. Many companies soon find themselves spending more time making their cloud applications secure and scalable than they do writing its core functionality. TinyStacks takes the drudgery out of deployment. Using TinyStacks, development teams can: Create a multi-stage (dev/test/stage/prod) CI/CD pipeline. TinyStacks deploys directly to your own AWS account, making it easy to deploy onto the cloud quickly while also providing transparency and flexibility. Use your favorite languages and frameworks Deploy applications to production to the cloud with minimal overhead Scale applications and APIs from hundreds of users to millions with little additional effort By deploying with TinyStacks, your team can spend less time building out DevOps architecture and more time building out the core features that make your application unique!","title":"Introduction"},{"location":"#what-tinystacks-does","text":"Below are some of the benefits that applications gain by deploying with TinyStacks. Use Your Favorite Languages and Frameworks. TinyStacks is language- and framework-agnostic. Our deployment processes will work with any application deployed via a Docker container. If your application currently isn\u2019t Dockerized, we have pre-built template files for the Node.js, Django, Express, and Flask frameworks to get you started. Full-Stack Architecture. TinyStacks creates everything your application requires to run on the cloud. It can even provision a Postgres database for data storage. Multi-Stage Deployments. TinyStacks supports defining as many development stages as you need. By default, we create a dev stage for development purposes. Using the TinyStacks dashboard, you can easily add a production stage and any intermediate stages - such as testing and staging - to your deployment process. Continuous Integration and Continuous Deployment. A TinyStacks stack is wired to a repository you own in GitHub or GitLab. By default, any changes you make to a stack\u2019s corresponding branch will be automatically compiled and updated in the currently running environment. You can also define manual approvals for any stage. Security by Default. TinyStacks incorporates the latest security best practices for cloud-based applications. Cost Efficiency. Controlling costs on the cloud is difficult. Finding the best price point for your application can require hundreds of hours of testing and experimentation. TinyStacks constantly monitor, test, and change the way we deploy stacks for our customers to get the best performance at the lowest price. (Read about a major change we made to our deployments that saved up to 40% on stack costs for our customers.)","title":"What TinyStacks Does"},{"location":"#what-tinystacks-doesnt-do","text":"While TinyStacks conveys many benefits to development teams, it can\u2019t do everything . The following areas remain your responsibility: Hosting. TinyStacks is a DevOps orchestration framework, not a cloud hosting provider. All provisioned cloud capacity is run in your own cloud provider\u2019s accounts and you are directly responsible for all costs incurred. End to End Security. TinyStacks cannot make an insecure application secure. While Tinystacks takes multiple measures to ensure your infrastructure is secure by default, it does not guarantee the security of your application. It is up to you and your team to design and implement applications that adhere to the tenets of good security, such as defensive programming and the principle of least privilege.","title":"What TinyStacks Doesn\u2019t Do"},{"location":"architecture/","text":"When you create a new stack with TinyStacks, we stand up a number of resources in your AWS account. This section describes how each AWS resource fits into your stack, as well as the average costs for each AWS resource. The role of containers A Docker container is a lightweight virtualized operating system that runs on top of other operating systems such as Linux or Windows. Before Docker, deploying applications at scale was fraught with difficulty. To run successfully, an application requires the correct configuration - the right version of its programming language runtime, shared libraries, and local state (environment variables, file system permissions, etc.). But system administrators were often forced to deploy applications across multiple servers with mismatched configurations. Docker simplifies deployment by packaging an application in a virtualized operating system that contains everything the application needs to run successfully, virtually eliminating the dependency issue. Once an application is verified to run successfully in its container, you can deploy as many running copies as you need to support your users at scale. You can also avoid bloat by shipping the application with exactly the dependencies it needs and nothing more. Two ways to run your application TinyStacks stores your built application as a container in Amazon Elastic Container Registry (ECR) . Storing your Docker images in Amazon ECR makes it easy to deploy running instances of the image. It also provides a historical repository of images that simplifies version rollback. From there, you have a choice as to how to deploy your application. Serverless A \"serverless\" application is one that doesn't require you to provision computing resources (such as Amazon EC2 instances) in your own AWS account. Instead, AWS simply takes your code (in this case, packaged as a Docker container) and runs it on an available computing instance that it maintains for you behind the scenes. TinyStacks serverless stacks run your application using AWS's original serverless technology, AWS Lambda . Serverless is a great option for pre-production stacks (dev stacks) as well as for production applications with bursting usage patterns - i.e., apps with punctuated periods of high and low activity. For more information, see Serverless . Elastic Container Service (ECS) ECS is a container orchestration service that enables running Docker containers quickly and easily. Containers can be run on a set of virtual machines (a cluster) that you run and manage in your own account. Alternatively, they may be run using AWS Fargate , a serverless component of Amazon ECS that runs containers on computing capacity managed by AWS. For most deployments, TinyStacks manages its own ECS clusters on behalf of our customers. For more information on how to control cluster configuration and scaling, see Compute . Architectural components A stack consists of an application written on a specific application framework (such as Express, Flask, Django, Spring, etc.) packaged inside of a Docker container. TinyStacks creates the necessary architecture inside of your AWS account to run this Dockerized application in a scalable and highly available architecture on AWS. The following diagram shows the major architectural components in a stack for a container architecture: Following are the components used for serverless architectures: GitHub . A source code repository application based on the source control program Git. Stores the code for your application as well as some ancillary files needed in the stack deployment process. (TinyStacks also supports using GitLab as one\u2019s repository.) AWS CodePipeline . A continuous integration and continuous deployment service. Responsible for coordinating the build and deployment of the Docker container. AWS CodeBuild . A managed continuous integration service that builds source code into deployable applications. Comprises the steps in the CodePipeline. Amazon Elastic Container Registry (ECR) . Docker container storage service. Stores the Dockerized version of the application. Amazon Elastic Container Service (ECS) . Docker container orchestration service. Runs the latest version of the application. AWS Lambda . Serverless execution technology for running code directly in the AWS cloud without standing up compute directly in your AWS account. Amazon CloudWatch . Amazon\u2019s metrics, monitoring, and alerting service. Provides some basic alerts around application health and build/release status. Amazon API Gateway . A service for creating managed Application Programming Interfaces (API) at scale. Creates endpoints to the API functions defined by the application residing in the Docker container. Amazon Application Load Balancer . An alternative to API Gateway that is more suitable for applications serving over one million requests/month. Networking AWS resources run in virtual networks called Virtual Private Clouds (VPCs). TinyStacks builds its own VPC for your application that is secure by default. Alternatively, you can use your own VPC that already exists in your AWS account, assuming it meets certain criteria. For more information, see Networking . Application flow GitHub stack creation When you create a new stack in TinyStacks, you can choose whether to use a starter project or use an existing project already defined in your GitHub account. When you use a starter project, TinyStacks creates a new repository in your GitHub account that contains a basic Create/Read/Update/Delete (CRUD) data application with a handful of REST API endpoints. It also creates two YAML files that are used by TinyStacks for the AWS CodePipeline portion of the deployment: build.yml . Builds the Docker container and pushes it into the Amazon ECR instance. release.yml . Updates the running service in your Amazon ECS cluster. If you use an existing GitHub project, you will need to find the build.yml and release.yml files in the relevant TinyStacks template in Github and check them into your existing Github repository. For more information, including information on how to manage \"monorepos\" (repositories containing multiple releases and multiple build/release files), see Builds and Releases . Continuous Integration with Code Pipeline Your GitHub repository is connected to an AWS CodePipeline project. TinyStacks uses AWS CodePipeline to enable: Continuous integration , which ensures that your project is always built, packaged, and ready to be deployed. Continuous deployment , which deploys the latest package of your running project into the target environment. When you create a stack, you specify which branch of your GitHub repository you wish to connect to your AWS CodePipeline project. Whenever you commit a change to this branch, an AWS Lambda function does a full clone of the repository, creates a new ZIP file for the application, and uploads it to an Amazon S3 bucket. This change to the key in Amazon S3 triggers AWS CodePipeline to build and deploy your latest changes. Pushing changes to Amazon ECR and Amazon ECS Your running application is hosted in Amazon ECS using an image stored in an Amazon ECR registry. TinyStacks creates both of these resources for you when it stands up your stack. Your ECS configuration consists of the following building blocks : An ECS cluster , a collection of virtual machines on which containers are run. TinyStacks manages its own ECS clusters on behalf of our customers\u2019 deployments. A task definition that specifies the container to run for your service. One or more running tasks , which correspond to a running instance of your container. A service , which ensures that a set number of running tasks are always accepting requests. By default, TinyStacks creates a single task for your service. However, this may be increased automatically up to five running tasks if your API is subject to high demand. (See below for more information on auto scaling.) Deployments through ECR and ECS are driven by your AWS CodePipeline project. This project contains two AWS CodeBuild projects: Build (specified by your project\u2019s build.yaml file). The Build project runs first and compiles a new Docker image containing your application\u2019s latest changes. It then pushes this new image up to the Amazon ECR container repository that TinyStacks created for your project. Release (specified by your project\u2019s release.yml file). After Build has run successfully, the Release project downloads and re-labels the current image pushed by the Build project to Amazon ECR. It then updates the running task definition on your Amazon Fargate cluster to ensure that your application is using the latest image. The output of both of these projects is available in the AWS CodePipeline console. TinyStacks also stores this output in Amazon CloudWatch Logs. Managed API endpoints By default, your Amazon ECS container is hosted on an instance in a public subnet. However, the instance itself does not have a public IP address. TinyStacks uses Amazon API Gateway or Application Load Balancer to provide a publicly accessible endpoint onto your container\u2019s REST API methods. Amazon API Gateway provides you with fine-grained control over your REST API with support for configuring authorization, usage throttling, and advanced request routing, among other features. Application Load Balancer also provides routing support in addition to balancing requests across resource targets to avoid overwhelming any single resource. When you create a stack, you have a choice to use either API Gateway or Application Load Balancer. For more details on the differences, which to choose, and how this effects your application, see Load Balancers . Amazon CloudWatch for auto scaling TinyStacks uses Amazon CloudWatch to set up metrics and alerts to help scale out and scale in your application in proportion to the traffic it's receiving. For more information, see Autoscaling . Cost of a TinyStacks stack Note: Prices based on pricing of services in the US East (Virginia) data center as of July 2021. Pricing is subject to change without notice. Prices do not include any relevant taxes. The TinyStacks stack is designed, not just for reliability and scalability, but for cost-effectiveness as well. We estimate that running a typical stack in a development capacity will cost around $46 per month. Note that your actual costs may vary based on a couple of factors: Free Tier . If you have a new AWS account, you can consume a set amount of key AWS services for free every month for the first 12 months of your account\u2019s lifetime. If you exceed the free tier limits, you will incur additional charges and your costs will exceed those documented here. Usage . The estimates cited above assume light usage in a development/test environment. High-volume, production applications will incur higher costs. As end user usage of your application increases, costs will increase concomitantly. This is because the cloud services that support your application will automatically scale up and out to meet the increased demand, thus using additional computing resources. The two factors driving costs are the amount of traffic your application handles and the number of virtual machines required in your Amazon ECS cluster. Traffic costs TinyStacks supports both standard scale (using Amazon API Gateway) and hyperscale (using Application Load Balancer) configurations. Your costs will scale differently on each configuration as your usage increases. In general, a hyperscale configuration will prove more cost-effective at a volume of 500,000 or more application requests per month. In a standard scale application, API Gateway costs will increase by around $1 for every additional million requests your application handles. In a hyperscale application, your costs will vary based on the number of Load Balancer Capacity Units (LCUs) your application consumes. LCUs are a complex metric, with a single LCU providing the following resources: 25 new connections per second 3,000 active connections every minute 1 GB of data sent to the Amazon EC2 instances in your Amazon ECS cluster 1,000 rules evaluated every second If your application exceeds any of these resource limits, you will pay for another LCU at a cost of around USD $0.008 per hour. You will pay for the largest number of LCUs required to accommodate your most demanding resource. For example, say that your application is receiving an average of 10,000 new connections every minute and is processing 3 GB of data every hour. In this instance, three LCUs would be enough to cover 3 GB of data. However, you will need 6.68 LCUs to cover 10,000 connections every minute (around 167 connections/second). According to the AWS Pricing Calculator , this would bring your LCU charges in us-east-1 up to around $40/month. Note that, in the above example, an application using API Gateway would be serving around 432 million connections per month for a total additional cost of $432/month. This makes it clear how much more economical it is to use Application Load Balancer when running at hyperscale. Amazon ECS cluster costs All Docker containers are hosted in an Amazon ECS cluster in your AWS account. An ECS cluster contains one or more instances of an Amazon EC2 virtual machine. By default, Tinystacks runs once EC2 instance in your cluster. As demand on your application grows, we will scale out and create a new cluster instance when the average CPU utilization across all cluster instances exceeds 70% for five minutes or longer. You can configure what instance size to use for your application when you create your stack. For standard scale applications, the t3.micro size is a good starter size that will limit your AWS spend (t3.micro instances will only run around USD $8/month). For hyperscale applications serving over 1 million requests/month, select m3.medium or m3.large depending on how CPU-intensive your workload is.","title":"Architecture"},{"location":"architecture/#the-role-of-containers","text":"A Docker container is a lightweight virtualized operating system that runs on top of other operating systems such as Linux or Windows. Before Docker, deploying applications at scale was fraught with difficulty. To run successfully, an application requires the correct configuration - the right version of its programming language runtime, shared libraries, and local state (environment variables, file system permissions, etc.). But system administrators were often forced to deploy applications across multiple servers with mismatched configurations. Docker simplifies deployment by packaging an application in a virtualized operating system that contains everything the application needs to run successfully, virtually eliminating the dependency issue. Once an application is verified to run successfully in its container, you can deploy as many running copies as you need to support your users at scale. You can also avoid bloat by shipping the application with exactly the dependencies it needs and nothing more.","title":"The role of containers"},{"location":"architecture/#two-ways-to-run-your-application","text":"TinyStacks stores your built application as a container in Amazon Elastic Container Registry (ECR) . Storing your Docker images in Amazon ECR makes it easy to deploy running instances of the image. It also provides a historical repository of images that simplifies version rollback. From there, you have a choice as to how to deploy your application.","title":"Two ways to run your application"},{"location":"architecture/#serverless","text":"A \"serverless\" application is one that doesn't require you to provision computing resources (such as Amazon EC2 instances) in your own AWS account. Instead, AWS simply takes your code (in this case, packaged as a Docker container) and runs it on an available computing instance that it maintains for you behind the scenes. TinyStacks serverless stacks run your application using AWS's original serverless technology, AWS Lambda . Serverless is a great option for pre-production stacks (dev stacks) as well as for production applications with bursting usage patterns - i.e., apps with punctuated periods of high and low activity. For more information, see Serverless .","title":"Serverless"},{"location":"architecture/#elastic-container-service-ecs","text":"ECS is a container orchestration service that enables running Docker containers quickly and easily. Containers can be run on a set of virtual machines (a cluster) that you run and manage in your own account. Alternatively, they may be run using AWS Fargate , a serverless component of Amazon ECS that runs containers on computing capacity managed by AWS. For most deployments, TinyStacks manages its own ECS clusters on behalf of our customers. For more information on how to control cluster configuration and scaling, see Compute .","title":"Elastic Container Service (ECS)"},{"location":"architecture/#architectural-components","text":"A stack consists of an application written on a specific application framework (such as Express, Flask, Django, Spring, etc.) packaged inside of a Docker container. TinyStacks creates the necessary architecture inside of your AWS account to run this Dockerized application in a scalable and highly available architecture on AWS. The following diagram shows the major architectural components in a stack for a container architecture: Following are the components used for serverless architectures: GitHub . A source code repository application based on the source control program Git. Stores the code for your application as well as some ancillary files needed in the stack deployment process. (TinyStacks also supports using GitLab as one\u2019s repository.) AWS CodePipeline . A continuous integration and continuous deployment service. Responsible for coordinating the build and deployment of the Docker container. AWS CodeBuild . A managed continuous integration service that builds source code into deployable applications. Comprises the steps in the CodePipeline. Amazon Elastic Container Registry (ECR) . Docker container storage service. Stores the Dockerized version of the application. Amazon Elastic Container Service (ECS) . Docker container orchestration service. Runs the latest version of the application. AWS Lambda . Serverless execution technology for running code directly in the AWS cloud without standing up compute directly in your AWS account. Amazon CloudWatch . Amazon\u2019s metrics, monitoring, and alerting service. Provides some basic alerts around application health and build/release status. Amazon API Gateway . A service for creating managed Application Programming Interfaces (API) at scale. Creates endpoints to the API functions defined by the application residing in the Docker container. Amazon Application Load Balancer . An alternative to API Gateway that is more suitable for applications serving over one million requests/month.","title":"Architectural components"},{"location":"architecture/#networking","text":"AWS resources run in virtual networks called Virtual Private Clouds (VPCs). TinyStacks builds its own VPC for your application that is secure by default. Alternatively, you can use your own VPC that already exists in your AWS account, assuming it meets certain criteria. For more information, see Networking .","title":"Networking"},{"location":"architecture/#application-flow","text":"","title":"Application flow"},{"location":"architecture/#github-stack-creation","text":"When you create a new stack in TinyStacks, you can choose whether to use a starter project or use an existing project already defined in your GitHub account. When you use a starter project, TinyStacks creates a new repository in your GitHub account that contains a basic Create/Read/Update/Delete (CRUD) data application with a handful of REST API endpoints. It also creates two YAML files that are used by TinyStacks for the AWS CodePipeline portion of the deployment: build.yml . Builds the Docker container and pushes it into the Amazon ECR instance. release.yml . Updates the running service in your Amazon ECS cluster. If you use an existing GitHub project, you will need to find the build.yml and release.yml files in the relevant TinyStacks template in Github and check them into your existing Github repository. For more information, including information on how to manage \"monorepos\" (repositories containing multiple releases and multiple build/release files), see Builds and Releases .","title":"GitHub stack creation"},{"location":"architecture/#continuous-integration-with-code-pipeline","text":"Your GitHub repository is connected to an AWS CodePipeline project. TinyStacks uses AWS CodePipeline to enable: Continuous integration , which ensures that your project is always built, packaged, and ready to be deployed. Continuous deployment , which deploys the latest package of your running project into the target environment. When you create a stack, you specify which branch of your GitHub repository you wish to connect to your AWS CodePipeline project. Whenever you commit a change to this branch, an AWS Lambda function does a full clone of the repository, creates a new ZIP file for the application, and uploads it to an Amazon S3 bucket. This change to the key in Amazon S3 triggers AWS CodePipeline to build and deploy your latest changes.","title":"Continuous Integration with Code Pipeline"},{"location":"architecture/#pushing-changes-to-amazon-ecr-and-amazon-ecs","text":"Your running application is hosted in Amazon ECS using an image stored in an Amazon ECR registry. TinyStacks creates both of these resources for you when it stands up your stack. Your ECS configuration consists of the following building blocks : An ECS cluster , a collection of virtual machines on which containers are run. TinyStacks manages its own ECS clusters on behalf of our customers\u2019 deployments. A task definition that specifies the container to run for your service. One or more running tasks , which correspond to a running instance of your container. A service , which ensures that a set number of running tasks are always accepting requests. By default, TinyStacks creates a single task for your service. However, this may be increased automatically up to five running tasks if your API is subject to high demand. (See below for more information on auto scaling.) Deployments through ECR and ECS are driven by your AWS CodePipeline project. This project contains two AWS CodeBuild projects: Build (specified by your project\u2019s build.yaml file). The Build project runs first and compiles a new Docker image containing your application\u2019s latest changes. It then pushes this new image up to the Amazon ECR container repository that TinyStacks created for your project. Release (specified by your project\u2019s release.yml file). After Build has run successfully, the Release project downloads and re-labels the current image pushed by the Build project to Amazon ECR. It then updates the running task definition on your Amazon Fargate cluster to ensure that your application is using the latest image. The output of both of these projects is available in the AWS CodePipeline console. TinyStacks also stores this output in Amazon CloudWatch Logs.","title":"Pushing changes to Amazon ECR and Amazon ECS"},{"location":"architecture/#managed-api-endpoints","text":"By default, your Amazon ECS container is hosted on an instance in a public subnet. However, the instance itself does not have a public IP address. TinyStacks uses Amazon API Gateway or Application Load Balancer to provide a publicly accessible endpoint onto your container\u2019s REST API methods. Amazon API Gateway provides you with fine-grained control over your REST API with support for configuring authorization, usage throttling, and advanced request routing, among other features. Application Load Balancer also provides routing support in addition to balancing requests across resource targets to avoid overwhelming any single resource. When you create a stack, you have a choice to use either API Gateway or Application Load Balancer. For more details on the differences, which to choose, and how this effects your application, see Load Balancers .","title":"Managed API endpoints"},{"location":"architecture/#amazon-cloudwatch-for-auto-scaling","text":"TinyStacks uses Amazon CloudWatch to set up metrics and alerts to help scale out and scale in your application in proportion to the traffic it's receiving. For more information, see Autoscaling .","title":"Amazon CloudWatch for auto scaling"},{"location":"architecture/#cost-of-a-tinystacks-stack","text":"Note: Prices based on pricing of services in the US East (Virginia) data center as of July 2021. Pricing is subject to change without notice. Prices do not include any relevant taxes. The TinyStacks stack is designed, not just for reliability and scalability, but for cost-effectiveness as well. We estimate that running a typical stack in a development capacity will cost around $46 per month. Note that your actual costs may vary based on a couple of factors: Free Tier . If you have a new AWS account, you can consume a set amount of key AWS services for free every month for the first 12 months of your account\u2019s lifetime. If you exceed the free tier limits, you will incur additional charges and your costs will exceed those documented here. Usage . The estimates cited above assume light usage in a development/test environment. High-volume, production applications will incur higher costs. As end user usage of your application increases, costs will increase concomitantly. This is because the cloud services that support your application will automatically scale up and out to meet the increased demand, thus using additional computing resources. The two factors driving costs are the amount of traffic your application handles and the number of virtual machines required in your Amazon ECS cluster.","title":"Cost of a TinyStacks stack"},{"location":"architecture/#traffic-costs","text":"TinyStacks supports both standard scale (using Amazon API Gateway) and hyperscale (using Application Load Balancer) configurations. Your costs will scale differently on each configuration as your usage increases. In general, a hyperscale configuration will prove more cost-effective at a volume of 500,000 or more application requests per month. In a standard scale application, API Gateway costs will increase by around $1 for every additional million requests your application handles. In a hyperscale application, your costs will vary based on the number of Load Balancer Capacity Units (LCUs) your application consumes. LCUs are a complex metric, with a single LCU providing the following resources: 25 new connections per second 3,000 active connections every minute 1 GB of data sent to the Amazon EC2 instances in your Amazon ECS cluster 1,000 rules evaluated every second If your application exceeds any of these resource limits, you will pay for another LCU at a cost of around USD $0.008 per hour. You will pay for the largest number of LCUs required to accommodate your most demanding resource. For example, say that your application is receiving an average of 10,000 new connections every minute and is processing 3 GB of data every hour. In this instance, three LCUs would be enough to cover 3 GB of data. However, you will need 6.68 LCUs to cover 10,000 connections every minute (around 167 connections/second). According to the AWS Pricing Calculator , this would bring your LCU charges in us-east-1 up to around $40/month. Note that, in the above example, an application using API Gateway would be serving around 432 million connections per month for a total additional cost of $432/month. This makes it clear how much more economical it is to use Application Load Balancer when running at hyperscale.","title":"Traffic costs"},{"location":"architecture/#amazon-ecs-cluster-costs","text":"All Docker containers are hosted in an Amazon ECS cluster in your AWS account. An ECS cluster contains one or more instances of an Amazon EC2 virtual machine. By default, Tinystacks runs once EC2 instance in your cluster. As demand on your application grows, we will scale out and create a new cluster instance when the average CPU utilization across all cluster instances exceeds 70% for five minutes or longer. You can configure what instance size to use for your application when you create your stack. For standard scale applications, the t3.micro size is a good starter size that will limit your AWS spend (t3.micro instances will only run around USD $8/month). For hyperscale applications serving over 1 million requests/month, select m3.medium or m3.large depending on how CPU-intensive your workload is.","title":"Amazon ECS cluster costs"},{"location":"autoscaling/","text":"TinyStacks runs two instances of your Amazon ECS container by default. However, if your application is under heavy load, these instances may not be enough to handle the demand. TinyStacks uses the auto scaling feature of Amazon ECS to ensure your application can still run under heavy demand. Your stack defines several AWS CloudWatch alarms that monitor CPU utilization on the running containers associated with your task definition. If aggregate utilization for all running containers exceeds 75%, another container is launched, up to a maximum of five. Note that the scalability of a container is dependent upon good programming practices. Your application code should avoid storing state on disk or in memory on any given container, as you cannot predict which running container instance will service a given request. Changing your cluster scale settings When you create a stage , you can specify how you want your application to scale. If you need to change these at any time, you can do so through the stage's Settings page. Go to your Stacks page and, for the stack you want to modify, click the gear icon in the lower right corner. By default, TinyStacks will start two Amazon EC2 instances in your ECS cluster. We also define Amazon CloudWatch rules that implement the following behavior: If the aggregate CPU usage of your ECS cluster instances exceeds 75% for more than 15 minutes, AWS will automatically add another EC2 instances to the cluster. If the aggregate CPU usage of your ECS cluster instances is lower than 75% for more than 15 minutes, AWS will automatically remove an EC2 instance from the cluster. You may need to change these defaults if your application is either using too many underutilized resources or (more likely) is maxing out on available ECS cluster instances. To change scale settings, select Build and scale from the navigation menu on the Settings page. Change EC2 instance sizing settings Here, you can change several settings. The first is your application's instance sizing . You can upgrade instance sizing if you are spinning up new ECS cluster instances too quickly. If you are currently using a t3.micro instance size, consider upgrading to an m3.medium or an m3.large. Change cluster scale settings You can also control how many EC2 instances your cluster runs. You can control the following three settings. Desired capacity The number of ECS cluster instances to maintain if no auto scaling rules have been triggered by a CloudWatch alarm. The desired capacity should be set to a level that can sustain normal, everyday traffic. If you set it too high, you will spend additional money on ECS cluster instances you aren't using. Conversely, if you set it too high, you may experience service interruptions, as it can take several minutes to initialize a new cluster instance. Min Capacity The minimum number of cluster instances to run regardless of auto scaling rules. As stated above, scaling up a new cluster instance can take time; setting a minimum number of instances ensures you always have \"warm\" cluster instances that are stood up and able to accept incoming traffic. Max Capacity The maximum number of cluster instances to run regardless of auto scaling rules. This setting provides an upper boundary beyond which your application will not scale, which is useful for controlling costs or responding to a potential denial of service attack. For more information on scaling of EC2 instances, see the AWS documentation . Note : Saving settings changes will trigger a rebuild of your stage. Your application may not be accessible on this stage until the rebuild and redeploy has completed. Changing any of the settings in this section may result in increased AWS service costs.","title":"Autoscaling"},{"location":"autoscaling/#changing-your-cluster-scale-settings","text":"When you create a stage , you can specify how you want your application to scale. If you need to change these at any time, you can do so through the stage's Settings page. Go to your Stacks page and, for the stack you want to modify, click the gear icon in the lower right corner. By default, TinyStacks will start two Amazon EC2 instances in your ECS cluster. We also define Amazon CloudWatch rules that implement the following behavior: If the aggregate CPU usage of your ECS cluster instances exceeds 75% for more than 15 minutes, AWS will automatically add another EC2 instances to the cluster. If the aggregate CPU usage of your ECS cluster instances is lower than 75% for more than 15 minutes, AWS will automatically remove an EC2 instance from the cluster. You may need to change these defaults if your application is either using too many underutilized resources or (more likely) is maxing out on available ECS cluster instances. To change scale settings, select Build and scale from the navigation menu on the Settings page.","title":"Changing your cluster scale settings"},{"location":"autoscaling/#change-ec2-instance-sizing-settings","text":"Here, you can change several settings. The first is your application's instance sizing . You can upgrade instance sizing if you are spinning up new ECS cluster instances too quickly. If you are currently using a t3.micro instance size, consider upgrading to an m3.medium or an m3.large.","title":"Change EC2 instance sizing settings"},{"location":"autoscaling/#change-cluster-scale-settings","text":"You can also control how many EC2 instances your cluster runs. You can control the following three settings. Desired capacity The number of ECS cluster instances to maintain if no auto scaling rules have been triggered by a CloudWatch alarm. The desired capacity should be set to a level that can sustain normal, everyday traffic. If you set it too high, you will spend additional money on ECS cluster instances you aren't using. Conversely, if you set it too high, you may experience service interruptions, as it can take several minutes to initialize a new cluster instance. Min Capacity The minimum number of cluster instances to run regardless of auto scaling rules. As stated above, scaling up a new cluster instance can take time; setting a minimum number of instances ensures you always have \"warm\" cluster instances that are stood up and able to accept incoming traffic. Max Capacity The maximum number of cluster instances to run regardless of auto scaling rules. This setting provides an upper boundary beyond which your application will not scale, which is useful for controlling costs or responding to a potential denial of service attack. For more information on scaling of EC2 instances, see the AWS documentation . Note : Saving settings changes will trigger a rebuild of your stage. Your application may not be accessible on this stage until the rebuild and redeploy has completed. Changing any of the settings in this section may result in increased AWS service costs.","title":"Change cluster scale settings"},{"location":"basic-settings/","text":"Your service contains several basic settings that you can access by clicking on the settings for your stack (the gear icon for your stack on the TinyStacks dashboard). From the settings page, under Stack settings, you will see a dropdown listing your services. By default, you will see the initial service you deployed listed here. If you deployed additional services (sidecars), you will be able to select them here and set different settings for each sidecar service. For more details, see Sidecars . Settings Repository branch The branch from which your service's container is built. Any check-ins to this branch will trigger an automatic rebuild and (unless manual deployment is enabled for a stage) a deployment through your stack's stages. Port The port through which your application is exposed on the Internet. By default, this is port 80. Custom health check The URL used to determine if your application's container instances are healthy or unhealthy. By default, this is /ping . You can change it to another path hosted on your application container. If you change the health check path, please keep a few things in mind: A ping check should return an HTTP 200 OK message if it is healthy. Responses should generally be fast so as not to run the risk of a request timeout. If your custom health check breaks, it could result in your application \"churning\" - i.e., in container instances being repeatedly stood up and torn down because they appear unhealthy. This will result in your application going offline. As such, it's best to keep your health checks short and simple.","title":"Basic settings"},{"location":"basic-settings/#settings","text":"","title":"Settings"},{"location":"basic-settings/#repository-branch","text":"The branch from which your service's container is built. Any check-ins to this branch will trigger an automatic rebuild and (unless manual deployment is enabled for a stage) a deployment through your stack's stages.","title":"Repository branch"},{"location":"basic-settings/#port","text":"The port through which your application is exposed on the Internet. By default, this is port 80.","title":"Port"},{"location":"basic-settings/#custom-health-check","text":"The URL used to determine if your application's container instances are healthy or unhealthy. By default, this is /ping . You can change it to another path hosted on your application container. If you change the health check path, please keep a few things in mind: A ping check should return an HTTP 200 OK message if it is healthy. Responses should generally be fast so as not to run the risk of a request timeout. If your custom health check breaks, it could result in your application \"churning\" - i.e., in container instances being repeatedly stood up and torn down because they appear unhealthy. This will result in your application going offline. As such, it's best to keep your health checks short and simple.","title":"Custom health check"},{"location":"builds-releases/","text":"As discussed in our architectural overview , TinyStacks uses two YAML files ingested by AWS CodeBuild to manage your builds and releases. build.yml The build.yml file is run only once per Docker container image. The build.yml file must tag an image with the tag :latest by the end of the build: version: 0.2 phases: build: commands: - aws ecr get-login-password | docker login --username AWS --password-stdin $ECR_ENDPOINT - docker build -t builtimage . - docker tag builtimage:latest $ECR_IMAGE_URL:latest - docker push $ECR_IMAGE_URL:latest You can apply as many additional tags as you like. The only restriction is that your tags cannot have the same name as one of the stages in your stack. Build stage environment variables created by TinyStacks TinyStacks defines the following build environment variables to each build. These are immutable and your build cannot override them: Environment variable Description S3_BUCKET The Amazon S3 bucket where logs and build artifacts are stored. ECR_ENDPOINT The Amazon ECR repository where your application's Docker container image is stored. ECR_IMAGE_URL The full URL to your container image. release.yml Your release.yml file is run with every stage deployment (dev, staging, prod, etc.). The default release.yml supplied by TinyStacks tags your image created in the last stage with the name of the current stage and prepares a new deployment in Amazon ECS: version: 0.2 phases: build: commands: - docker login -u AWS -p $(aws ecr get-login-password --region $AWS_REGION) $ECR_ENDPOINT - docker pull $ECR_IMAGE_URL:$PREVIOUS_STAGE_NAME - docker tag $ECR_IMAGE_URL:$PREVIOUS_STAGE_NAME $ECR_IMAGE_URL:$STAGE_NAME - docker push $ECR_IMAGE_URL:$STAGE_NAME post_build: on-failure: CONTINUE commands: - region=\"${STAGE_REGION:-$AWS_REGION}\" - if [ -z \"$SERVICE_NAME\" ] || [ \"$SERVICE_NAME\" == \"placeholder\" ]; then echo 'Service is not ready yet. Repository tagged correctly, exiting now'; else aws ecs update-service --service $SERVICE_NAME --cluster $CLUSTER_ARN --region $region --force-new-deployment; fi Release stage environment variables created by TinyStacks TinyStacks defines the following release environment variables to each build. These are immutable and your release cannot override them: Environment variable Description S3_BUCKET The Amazon S3 bucket where logs and build artifacts are stored. STAGE_NAME The name of the current stage. ECR_ENDPOINT The Amazon ECR repository where your application's Docker container image is stored. ECR_IMAGE_URL The full URL to your container image. PREVIOUS_STAGE_NAME Name of the previous stage in your stack. For the first stage in a stack, this will always be :latest . CLUSTER_ARN The Amazon Resource Name (ARN) that uniquely identifies the Amazon ECS cluster that TinyStacks creates to run and scale your application. SERVICE_NAME The name of the ECS service that runs and scales your container image. STAGE_REGION The AWS region in which this stage is being launched. When are deployments run? Deployments are run whenever you check in a change from Git. TinyStacks will also start a deployment update whenever you add a new build environment variable or runtime environment variable to your stack or one of its stages. Managing multiple projects in a single repository (\"monorepo\") For ease of management, many teams prefer to maintain multiple projects in a single repository, creating a so-called \"monorepo\". For example, a team may decide to construct its application as a set of microservices, with each service stored in a subdirectory within a single Git repo. To use TinyStacks with monorepos, create one TinyStacks stack for each service in your monorepo. Next, you will need to add as build environment variable called DOCKERFILE_PATH to each stack that points to the relative path to your Dockerfile. Finally, modify your build.yml so that it builds your service using this file path: - docker build -t builtimage . -f $DOCKERFILE_PATH For more information, see Environment variables .","title":"Builds and releases"},{"location":"builds-releases/#buildyml","text":"The build.yml file is run only once per Docker container image. The build.yml file must tag an image with the tag :latest by the end of the build: version: 0.2 phases: build: commands: - aws ecr get-login-password | docker login --username AWS --password-stdin $ECR_ENDPOINT - docker build -t builtimage . - docker tag builtimage:latest $ECR_IMAGE_URL:latest - docker push $ECR_IMAGE_URL:latest You can apply as many additional tags as you like. The only restriction is that your tags cannot have the same name as one of the stages in your stack.","title":"build.yml"},{"location":"builds-releases/#build-stage-environment-variables-created-by-tinystacks","text":"TinyStacks defines the following build environment variables to each build. These are immutable and your build cannot override them: Environment variable Description S3_BUCKET The Amazon S3 bucket where logs and build artifacts are stored. ECR_ENDPOINT The Amazon ECR repository where your application's Docker container image is stored. ECR_IMAGE_URL The full URL to your container image.","title":"Build stage environment variables created by TinyStacks"},{"location":"builds-releases/#releaseyml","text":"Your release.yml file is run with every stage deployment (dev, staging, prod, etc.). The default release.yml supplied by TinyStacks tags your image created in the last stage with the name of the current stage and prepares a new deployment in Amazon ECS: version: 0.2 phases: build: commands: - docker login -u AWS -p $(aws ecr get-login-password --region $AWS_REGION) $ECR_ENDPOINT - docker pull $ECR_IMAGE_URL:$PREVIOUS_STAGE_NAME - docker tag $ECR_IMAGE_URL:$PREVIOUS_STAGE_NAME $ECR_IMAGE_URL:$STAGE_NAME - docker push $ECR_IMAGE_URL:$STAGE_NAME post_build: on-failure: CONTINUE commands: - region=\"${STAGE_REGION:-$AWS_REGION}\" - if [ -z \"$SERVICE_NAME\" ] || [ \"$SERVICE_NAME\" == \"placeholder\" ]; then echo 'Service is not ready yet. Repository tagged correctly, exiting now'; else aws ecs update-service --service $SERVICE_NAME --cluster $CLUSTER_ARN --region $region --force-new-deployment; fi","title":"release.yml"},{"location":"builds-releases/#release-stage-environment-variables-created-by-tinystacks","text":"TinyStacks defines the following release environment variables to each build. These are immutable and your release cannot override them: Environment variable Description S3_BUCKET The Amazon S3 bucket where logs and build artifacts are stored. STAGE_NAME The name of the current stage. ECR_ENDPOINT The Amazon ECR repository where your application's Docker container image is stored. ECR_IMAGE_URL The full URL to your container image. PREVIOUS_STAGE_NAME Name of the previous stage in your stack. For the first stage in a stack, this will always be :latest . CLUSTER_ARN The Amazon Resource Name (ARN) that uniquely identifies the Amazon ECS cluster that TinyStacks creates to run and scale your application. SERVICE_NAME The name of the ECS service that runs and scales your container image. STAGE_REGION The AWS region in which this stage is being launched.","title":"Release stage environment variables created by TinyStacks"},{"location":"builds-releases/#when-are-deployments-run","text":"Deployments are run whenever you check in a change from Git. TinyStacks will also start a deployment update whenever you add a new build environment variable or runtime environment variable to your stack or one of its stages.","title":"When are deployments run?"},{"location":"builds-releases/#managing-multiple-projects-in-a-single-repository-monorepo","text":"For ease of management, many teams prefer to maintain multiple projects in a single repository, creating a so-called \"monorepo\". For example, a team may decide to construct its application as a set of microservices, with each service stored in a subdirectory within a single Git repo. To use TinyStacks with monorepos, create one TinyStacks stack for each service in your monorepo. Next, you will need to add as build environment variable called DOCKERFILE_PATH to each stack that points to the relative path to your Dockerfile. Finally, modify your build.yml so that it builds your service using this file path: - docker build -t builtimage . -f $DOCKERFILE_PATH For more information, see Environment variables .","title":"Managing multiple projects in a single repository (\"monorepo\")"},{"location":"comparisons/","text":"In today\u2019s marketplace, you have no limit of choices when it comes to application deployment. We believe that TinyStacks offers the most transparent and flexible path for small shops and teams looking to grow up and out. Here\u2019s a short summary of how we stack up (pun absolutely intended) against other solutions. Amazon Web Services (AWS) We\u2019re big fans of AWS at TinyStacks. Our founders are former engineering leaders at AWS. Our own service - and the services we deploy for customers - depend on it. In terms of features and scalability, AWS is the undisputed leader in public cloud technology. But while AWS is great, onboarding to AWS can be challenging - especially for smaller shops. Its web of overlapping features can take weeks or even months to understand, let alone master. Bringing an application onto AWS in a scalable, secure manner typically requires at least one dedicated DevOps engineer. AWS has taken numerous steps in response to complaints about its complexity. One of the most popular is its Cloud Development Kit (CDK), a set of command-line tools intended to ease onboarding to AWS. However, the CDK supposes an existing knowledge of AWS services. It also currently supports only a small number of languages. TinyStacks aims to simplify your team\u2019s onboarding to AWS. Using TinyStacks, you can bring your existing app into the cloud in a matter of days or weeks as opposed to months. And, as your team\u2019s knowledge of AWS grows, you\u2019re free to incorporate the service\u2019s rich features into your own applications - all while using TinyStacks to deploy and monitor your stacks easily. AWS Amplify AWS Amplify is an AWS feature geared towards creating Web and mobile applications. Amplify supports creating both back-end resources - databases, authentication, and storage - along with static Web and mobile front ends that connect to them. Amplify may be the right choice for some projects. But it brings serious overhead that may inhibit its suitability for others. Amplify takes a walled garden approach to application development. It steers developers towards certain architectural choices - such as Amazon Cognito for authentication and DynamoDB for data storage - that may not be right for your app. Additionally, Amplify ships with a weighty (2MB) client library that could negatively impact application load times. Finally, AWS Amplify\u2019s pricing model somehow manages to make AWS pricing even more complicated. Unlike Amplify, the TinyStacks DevOps workflow can be used with a variety of development stacks. We also don\u2019t dictate critical architectural decisions such as data storage and authentication. Amazon Lightsail Lightsail is Amazon\u2019s Platform as a Service (PaaS) offering, enabling developers to deploy out- of-the-box packaged applications (like WordPress) and common development stacks with a few button clicks. Like other PaaS offerings, Lightsail works on a fixed-cost model, meaning that monthly charges are steady and predictable. However, Lightsail is only easy to use for the easy stuff. Lightsail deployments are based on creating individual Amazon EC2 instances. Lightsail deployments don\u2019t implement horizontal scaling by default, meaning they won\u2019t add new instances automatically to handle usage spikes. Setting up a horizontal scaling deployment with a load balancer on Lightsail is almost as involved as performing the same task on native AWS. TinyStacks combines ease of use with out-of-the-box scalability. A stack created by TinyStacks will always scale to meet the needs of your business. With a few clicks, you can deploy scalable architecture on AWS while only paying for what you use. Heroku Heroku is a PaaS provider, itself built on AWS, that makes it easy to deploy applications with low computational workloads. Teams that start with Heroku can have great success with the platform...for a while. But as your application grows and becomes more complex, it may start to outstrip the capabilities that Heroku offers. TinyStacks provides an easy-to-use bridge between services like Heroku and AWS. With TinyStacks, you can migrate your Heroku apps to the cloud and gain the following benefits: Performance . While Heroku performs well for small computational loads, many teams struggle with performance as their workloads and traffic increase. TinyStacks uses services such as Amazon ECS and API Gateway to ensure your application can scale to meet demand. Services . Heroku only exposes a fraction of AWS\u2019s rich feature set. TinyStacks provides the ease of use of a service like Heroku combined with the full power of AWS. You can start simple with your stack and slowly expand your application\u2019s features to take advantage of the full range of services that AWS has to offer. Cost . Heroku becomes increasingly expensive as your application scales out. TinyStacks works relentlessly to ensure you\u2019re getting the most cost-effective solution with your AWS stack deployments, no matter your application\u2019s size and scale. DigitalOcean Like Heroku, DigitalOcean can be a good choice for development teams when they start out. DigitalOcean\u2019s App Platform simplifies code deployment. And its fixed-price pricing model is alluring for firms just dipping their toes into the cloud. However, like Heroku, DigitalOcean offers a minimalistic set of features. As your business grows, it\u2019s likely you\u2019ll outgrow DigitalOcean. Plus, DigitalOcean doesn\u2019t provide any DevOps support or tools to manage application pipelines. If you want to have separate dev/test environments distinct from production or want to stand up a demo site for prospective customers, you\u2019ll need to roll it yourself. TinyStacks provides the ease of use of a platform like DigitalOcean backed by the power of AWS. Additionally, we make it easy to create and manage a full DevOps pipeline with separate dev, test, and prod environments - all without needing to hire a full-time DevOps engineer. Render Render is a Heroku competitor that provides a streamlined experience for deploying apps for a wide variety of developer frameworks. The company offers multiple higher-level features, such as Content Delivery Network (CDN) support and HTTP health checks. Like Heroku, however, Render deploys cloud resources on your behalf. The lack of transparent access to an underlying cloud provider limits growth. Eventually, you will run into scenarios that require access to the full capabilities of a service such as AWS. TinyStacks was built to make cloud deployment easier - not to limit your application\u2019s growth. Since TinyStacks applications run in your own AWS account, you retain full control and visibility over them. Managed Kubernetes Another option your team might be weighing is a managed Kubernetes approach. Kubernetes (\u201ck8s\u201d) is a popular container orchestration and management system that works with Docker and other containerization technologies. It\u2019s gained such a reputation for its scalability and reliability that every major cloud provider and many PaaS providers offer some form of managed Kubernetes hosting. The problem with Kubernetes? Complexity. For a team just transforming itself into a DevOps operation, Kubernetes has a daunting ramp-up time. It can take existing engineers months to ramp up on the system - and engineers with existing Kubernetes experience aren\u2019t cheap. Some PaaS-style Kubernetes solutions have managed to tame the complexity issue (somewhat) for first-time k8s developers. However, making them production-ready is often still a complex task that involves understanding and configuring multiple pieces of networking, logging, and monitoring software. And most managed k8s solutions don\u2019t solve the issue of standing up a full DevOps pipeline to flow changes between dev, test, and production. Finally, managed Kubernetes solutions tend to be pricey. For example, using AWS\u2019s managed solution, Elastic Kubernetes Service (EKS), will add an additional hourly cost on top of the underlying EC2 capacity used. TinyStacks does more than just simplify deployment - it supports a full DevOps architecture for growing teams. With a few button clicks, your team can create multiple stacks and bring additional rigor and reliability to your release process.","title":"How TinyStacks stacks up"},{"location":"comparisons/#amazon-web-services-aws","text":"We\u2019re big fans of AWS at TinyStacks. Our founders are former engineering leaders at AWS. Our own service - and the services we deploy for customers - depend on it. In terms of features and scalability, AWS is the undisputed leader in public cloud technology. But while AWS is great, onboarding to AWS can be challenging - especially for smaller shops. Its web of overlapping features can take weeks or even months to understand, let alone master. Bringing an application onto AWS in a scalable, secure manner typically requires at least one dedicated DevOps engineer. AWS has taken numerous steps in response to complaints about its complexity. One of the most popular is its Cloud Development Kit (CDK), a set of command-line tools intended to ease onboarding to AWS. However, the CDK supposes an existing knowledge of AWS services. It also currently supports only a small number of languages. TinyStacks aims to simplify your team\u2019s onboarding to AWS. Using TinyStacks, you can bring your existing app into the cloud in a matter of days or weeks as opposed to months. And, as your team\u2019s knowledge of AWS grows, you\u2019re free to incorporate the service\u2019s rich features into your own applications - all while using TinyStacks to deploy and monitor your stacks easily.","title":"Amazon Web Services (AWS)"},{"location":"comparisons/#aws-amplify","text":"AWS Amplify is an AWS feature geared towards creating Web and mobile applications. Amplify supports creating both back-end resources - databases, authentication, and storage - along with static Web and mobile front ends that connect to them. Amplify may be the right choice for some projects. But it brings serious overhead that may inhibit its suitability for others. Amplify takes a walled garden approach to application development. It steers developers towards certain architectural choices - such as Amazon Cognito for authentication and DynamoDB for data storage - that may not be right for your app. Additionally, Amplify ships with a weighty (2MB) client library that could negatively impact application load times. Finally, AWS Amplify\u2019s pricing model somehow manages to make AWS pricing even more complicated. Unlike Amplify, the TinyStacks DevOps workflow can be used with a variety of development stacks. We also don\u2019t dictate critical architectural decisions such as data storage and authentication.","title":"AWS Amplify"},{"location":"comparisons/#amazon-lightsail","text":"Lightsail is Amazon\u2019s Platform as a Service (PaaS) offering, enabling developers to deploy out- of-the-box packaged applications (like WordPress) and common development stacks with a few button clicks. Like other PaaS offerings, Lightsail works on a fixed-cost model, meaning that monthly charges are steady and predictable. However, Lightsail is only easy to use for the easy stuff. Lightsail deployments are based on creating individual Amazon EC2 instances. Lightsail deployments don\u2019t implement horizontal scaling by default, meaning they won\u2019t add new instances automatically to handle usage spikes. Setting up a horizontal scaling deployment with a load balancer on Lightsail is almost as involved as performing the same task on native AWS. TinyStacks combines ease of use with out-of-the-box scalability. A stack created by TinyStacks will always scale to meet the needs of your business. With a few clicks, you can deploy scalable architecture on AWS while only paying for what you use.","title":"Amazon Lightsail"},{"location":"comparisons/#heroku","text":"Heroku is a PaaS provider, itself built on AWS, that makes it easy to deploy applications with low computational workloads. Teams that start with Heroku can have great success with the platform...for a while. But as your application grows and becomes more complex, it may start to outstrip the capabilities that Heroku offers. TinyStacks provides an easy-to-use bridge between services like Heroku and AWS. With TinyStacks, you can migrate your Heroku apps to the cloud and gain the following benefits: Performance . While Heroku performs well for small computational loads, many teams struggle with performance as their workloads and traffic increase. TinyStacks uses services such as Amazon ECS and API Gateway to ensure your application can scale to meet demand. Services . Heroku only exposes a fraction of AWS\u2019s rich feature set. TinyStacks provides the ease of use of a service like Heroku combined with the full power of AWS. You can start simple with your stack and slowly expand your application\u2019s features to take advantage of the full range of services that AWS has to offer. Cost . Heroku becomes increasingly expensive as your application scales out. TinyStacks works relentlessly to ensure you\u2019re getting the most cost-effective solution with your AWS stack deployments, no matter your application\u2019s size and scale.","title":"Heroku"},{"location":"comparisons/#digitalocean","text":"Like Heroku, DigitalOcean can be a good choice for development teams when they start out. DigitalOcean\u2019s App Platform simplifies code deployment. And its fixed-price pricing model is alluring for firms just dipping their toes into the cloud. However, like Heroku, DigitalOcean offers a minimalistic set of features. As your business grows, it\u2019s likely you\u2019ll outgrow DigitalOcean. Plus, DigitalOcean doesn\u2019t provide any DevOps support or tools to manage application pipelines. If you want to have separate dev/test environments distinct from production or want to stand up a demo site for prospective customers, you\u2019ll need to roll it yourself. TinyStacks provides the ease of use of a platform like DigitalOcean backed by the power of AWS. Additionally, we make it easy to create and manage a full DevOps pipeline with separate dev, test, and prod environments - all without needing to hire a full-time DevOps engineer.","title":"DigitalOcean"},{"location":"comparisons/#render","text":"Render is a Heroku competitor that provides a streamlined experience for deploying apps for a wide variety of developer frameworks. The company offers multiple higher-level features, such as Content Delivery Network (CDN) support and HTTP health checks. Like Heroku, however, Render deploys cloud resources on your behalf. The lack of transparent access to an underlying cloud provider limits growth. Eventually, you will run into scenarios that require access to the full capabilities of a service such as AWS. TinyStacks was built to make cloud deployment easier - not to limit your application\u2019s growth. Since TinyStacks applications run in your own AWS account, you retain full control and visibility over them.","title":"Render"},{"location":"comparisons/#managed-kubernetes","text":"Another option your team might be weighing is a managed Kubernetes approach. Kubernetes (\u201ck8s\u201d) is a popular container orchestration and management system that works with Docker and other containerization technologies. It\u2019s gained such a reputation for its scalability and reliability that every major cloud provider and many PaaS providers offer some form of managed Kubernetes hosting. The problem with Kubernetes? Complexity. For a team just transforming itself into a DevOps operation, Kubernetes has a daunting ramp-up time. It can take existing engineers months to ramp up on the system - and engineers with existing Kubernetes experience aren\u2019t cheap. Some PaaS-style Kubernetes solutions have managed to tame the complexity issue (somewhat) for first-time k8s developers. However, making them production-ready is often still a complex task that involves understanding and configuring multiple pieces of networking, logging, and monitoring software. And most managed k8s solutions don\u2019t solve the issue of standing up a full DevOps pipeline to flow changes between dev, test, and production. Finally, managed Kubernetes solutions tend to be pricey. For example, using AWS\u2019s managed solution, Elastic Kubernetes Service (EKS), will add an additional hourly cost on top of the underlying EC2 capacity used. TinyStacks does more than just simplify deployment - it supports a full DevOps architecture for growing teams. With a few button clicks, your team can create multiple stacks and bring additional rigor and reliability to your release process.","title":"Managed Kubernetes"},{"location":"compute/","text":"As discussed on our Architecture page, TinyStacks uses Amazon Elastic Container Service (ECS) to host your application's Docker containers. Your ECS Cluster On AWS, containers can be hosted in one of two ways: On an ECS cluster (a group of Amazon ECS virtual machine instances) hosted on your AWS account; or Via AWS Fargate, a fully managed container hosting service. Fargate still runs your container on clusters of EC2 instances. However, these instances are managed by AWS and are never visible in your account. In Fargate, your container may run on the same EC2 instances as containers launched by other AWS customers. TinyStacks uses EC2 clusters hosted in your own AWS account. The primary considering behind this decision was cost. Our own experiments showed that using EC2 instances resulted in a 40% cost savings to our customers. ECS services There are two ways to run containers on ECS. One is to run individual tasks, or running instances of your container. The other is to run your container as a service. An ECS service runs a specified number of instances of your container on your cluster. Using a service provides an additional level of quality for your application. If a container task fails and stops running, ECS can detect this and replace it with a new, healthy instance. ECS services also contain logic to limit container re-spawn and thus reduce thrashing. TinyStacks uses the REPLICA service scheduler strategy, which spreads task instances evenly across cluster instances and AWS availability zones (AZs). This ensures high application reliability should a single AZ become unavailable. For more information, see the AWS documentation on services . Running and updating your container Your container is defined using a task definition , a versioned specification that defines various parameters for your running container. When you publish a new version of your application, TinyStacks creates a new version of the task definition and publishes it to the service. Your service will gradually spin down instances of its old container and spin up instances of the new one in order to prevent any service interruptions. Scaling You can configure your application to scale to handle increased load in one of two ways: Scaling out the number of Amazon EC2 instances running in your ECS cluster. Scaling out the number of running instances of your container. For detailed information, see Autoscaling .","title":"Compute"},{"location":"compute/#your-ecs-cluster","text":"On AWS, containers can be hosted in one of two ways: On an ECS cluster (a group of Amazon ECS virtual machine instances) hosted on your AWS account; or Via AWS Fargate, a fully managed container hosting service. Fargate still runs your container on clusters of EC2 instances. However, these instances are managed by AWS and are never visible in your account. In Fargate, your container may run on the same EC2 instances as containers launched by other AWS customers. TinyStacks uses EC2 clusters hosted in your own AWS account. The primary considering behind this decision was cost. Our own experiments showed that using EC2 instances resulted in a 40% cost savings to our customers.","title":"Your ECS Cluster"},{"location":"compute/#ecs-services","text":"There are two ways to run containers on ECS. One is to run individual tasks, or running instances of your container. The other is to run your container as a service. An ECS service runs a specified number of instances of your container on your cluster. Using a service provides an additional level of quality for your application. If a container task fails and stops running, ECS can detect this and replace it with a new, healthy instance. ECS services also contain logic to limit container re-spawn and thus reduce thrashing. TinyStacks uses the REPLICA service scheduler strategy, which spreads task instances evenly across cluster instances and AWS availability zones (AZs). This ensures high application reliability should a single AZ become unavailable. For more information, see the AWS documentation on services .","title":"ECS services"},{"location":"compute/#running-and-updating-your-container","text":"Your container is defined using a task definition , a versioned specification that defines various parameters for your running container. When you publish a new version of your application, TinyStacks creates a new version of the task definition and publishes it to the service. Your service will gradually spin down instances of its old container and spin up instances of the new one in order to prevent any service interruptions.","title":"Running and updating your container"},{"location":"compute/#scaling","text":"You can configure your application to scale to handle increased load in one of two ways: Scaling out the number of Amazon EC2 instances running in your ECS cluster. Scaling out the number of running instances of your container. For detailed information, see Autoscaling .","title":"Scaling"},{"location":"concepts/","text":"The following page defines the key concepts you need to know to use TinyStacks effectively. Standing Up a DevOps Infrastructure To deploy an application, development teams must define two major components. First, they need a way to stand up the cloud infrastructure needed to run their application. Second, they need a way to stand up this infrastructure, on-demand, whenever they have a feature or bug fix to ship. Infrastructure as Code Before the advent of the cloud, companies had to stand up physical infrastructure - servers in data centers connected to high-speed networks - in order to deploy Web-based applications. This changed with the advent of cloud service providers such as AWS, which provide computing capacity on demand. Instead of standing up permanent capacity, companies could now rent what they needed, shut it down when they were finished, and pay for only what they used. Cloud computing capacity can be activated manually - e.g., via the AWS Management Console. It can also be activated automatically - via a programming language such as Python, or a declarative language such as AWS CloudFormation . This paves the way for infrastructure as code : the ability to provision the computing capacity an application requires in an automated, repeatable fashion. With infrastructure as code, a development team can deploy multiple releases of their application using the same template. Additionally, they can stand up and shut down environments with ease: a test environment can be spun up to verify an application\u2019s integrity, and then shut down at the end of the testing run to conserve costs. A team can also deploy multiple types of environments - from an environment for developers to a full production-ready deployment - using the same code. Continuous Integration and Continuous Deployment As discussed in the Architectural overview, TinyStacks aims to simplify two processes critical to rapid application development: continuous integration and continuous deployment . In continuous integration, changes to an application\u2019s codebase are compiled immediately once they are committed to a branch in a repository. This ensures that the codebase is healthy and can produce a viable release. In continuous deployment, the output produced by continuous integration is made available for usage and testing. Continuous deployment leverages infrastructure as code to deploy, not just the application, but all the underlying computing capacity the application requires. The goal of continuous deployment is to put a new, verified release into customer\u2019s hands as quickly as possible. The process used to compile and bundle an application and push it through various phases of testing is called a release pipeline . TinyStacks Automates The Release Process Both infrastructure as code and release pipelines are powerful concepts. The problem is that, historically, they both take considerable time and resources to implement. TinyStacks automates and simplifies the task of building a full end-to-end release process. It leverages infrastructure as code to deploy your applications with all of the cloud computing capacity and capabilities that it requires. Further, it provides a simple, intuitive user interface that you and your team can use to construct your own release pipelines. Services A service corresponds to a traditional microservice or Web application. Stacks A stack is a TinyStacks project that consists of a microservice or other Web application. A stack contains at least one stage (see below) and often contains multiple stages. Stages A stage is a deployment intended for testing or use at a specific point in the development process. A release pipeline typically consists of multiple stages. Development teams use stages to perform quality verification of a release before making it available to customers. While the type and number of stages will differ from team to team (or even project to project), a typical split is: A dev stage for testing brand new feature additions or bug fixes that work locally on a developer\u2019s environment. A test stage where automated test suites can be run alongside manual testing by other team members or internal stakeholders. A staging or preprod (pre-production) stage that mimics production and is used by internal employees and select customers to vet the final release. A prod (production) stage that hosts the customer-facing version of your application. For more on how to configure and manage stages, see Stages .","title":"Concepts"},{"location":"concepts/#standing-up-a-devops-infrastructure","text":"To deploy an application, development teams must define two major components. First, they need a way to stand up the cloud infrastructure needed to run their application. Second, they need a way to stand up this infrastructure, on-demand, whenever they have a feature or bug fix to ship.","title":"Standing Up a DevOps Infrastructure"},{"location":"concepts/#infrastructure-as-code","text":"Before the advent of the cloud, companies had to stand up physical infrastructure - servers in data centers connected to high-speed networks - in order to deploy Web-based applications. This changed with the advent of cloud service providers such as AWS, which provide computing capacity on demand. Instead of standing up permanent capacity, companies could now rent what they needed, shut it down when they were finished, and pay for only what they used. Cloud computing capacity can be activated manually - e.g., via the AWS Management Console. It can also be activated automatically - via a programming language such as Python, or a declarative language such as AWS CloudFormation . This paves the way for infrastructure as code : the ability to provision the computing capacity an application requires in an automated, repeatable fashion. With infrastructure as code, a development team can deploy multiple releases of their application using the same template. Additionally, they can stand up and shut down environments with ease: a test environment can be spun up to verify an application\u2019s integrity, and then shut down at the end of the testing run to conserve costs. A team can also deploy multiple types of environments - from an environment for developers to a full production-ready deployment - using the same code.","title":"Infrastructure as Code"},{"location":"concepts/#continuous-integration-and-continuous-deployment","text":"As discussed in the Architectural overview, TinyStacks aims to simplify two processes critical to rapid application development: continuous integration and continuous deployment . In continuous integration, changes to an application\u2019s codebase are compiled immediately once they are committed to a branch in a repository. This ensures that the codebase is healthy and can produce a viable release. In continuous deployment, the output produced by continuous integration is made available for usage and testing. Continuous deployment leverages infrastructure as code to deploy, not just the application, but all the underlying computing capacity the application requires. The goal of continuous deployment is to put a new, verified release into customer\u2019s hands as quickly as possible. The process used to compile and bundle an application and push it through various phases of testing is called a release pipeline .","title":"Continuous Integration and Continuous Deployment"},{"location":"concepts/#tinystacks-automates-the-release-process","text":"Both infrastructure as code and release pipelines are powerful concepts. The problem is that, historically, they both take considerable time and resources to implement. TinyStacks automates and simplifies the task of building a full end-to-end release process. It leverages infrastructure as code to deploy your applications with all of the cloud computing capacity and capabilities that it requires. Further, it provides a simple, intuitive user interface that you and your team can use to construct your own release pipelines.","title":"TinyStacks Automates The Release Process"},{"location":"concepts/#services","text":"A service corresponds to a traditional microservice or Web application.","title":"Services"},{"location":"concepts/#stacks","text":"A stack is a TinyStacks project that consists of a microservice or other Web application. A stack contains at least one stage (see below) and often contains multiple stages.","title":"Stacks"},{"location":"concepts/#stages","text":"A stage is a deployment intended for testing or use at a specific point in the development process. A release pipeline typically consists of multiple stages. Development teams use stages to perform quality verification of a release before making it available to customers. While the type and number of stages will differ from team to team (or even project to project), a typical split is: A dev stage for testing brand new feature additions or bug fixes that work locally on a developer\u2019s environment. A test stage where automated test suites can be run alongside manual testing by other team members or internal stakeholders. A staging or preprod (pre-production) stage that mimics production and is used by internal employees and select customers to vet the final release. A prod (production) stage that hosts the customer-facing version of your application. For more on how to configure and manage stages, see Stages .","title":"Stages"},{"location":"create-stack/","text":"The following guide is for first-time TinyStacks users. Follow these instructions to launch one of our sample applications on AWS in under 15 minutes! Prerequisites Before you begin this guide, make sure you have set up your AWS and Git service connections as outlined in Configure AWS and Git service connections . Select a starter project Next, you'll be prompted to select a project. You can either select a TinyStacks starter project or use your own project. For this guide, we'll be using a starter project, so go ahead and click the Starter projects button. The TinyStacks starter projects are simple Create/Read/Update/Delete (CRUD) database applications written in the application framework of your choice. You can click the name of any starter project in the window below to see its repository on GitHub and check out its README, which gives a detailed description of the app's code and deployment architecture. Once you select a project, TinyStacks will copy its code into your Git account, where you can use it as a basis for further development. For this guide, let's create an Express project . Next to Express in the window above, click Deploy . On the next screen, you'll be prompted to give your project a name. Choose a simple, short name that's 20 characters or less. (The name will be used as a prefix for some of the resources in your AWS account, so we want to keep it short to avoid going over naming length limits in AWS.) Choosing serverless or container deployment You have one more step to go and then you're ready to launch your stack! After clicking Deploy , you'll see the screen below. Here, you need to select whether to launch your application using either serverless or container architecture. To keep costs low for this initial walkthrough, select Serverless . (For more information on which architecture to choose for production applications, see Architecture and Serverless .) Further customizing your deployment This screen visualizes all of the AWS resources that TinyStacks will use or create on your behalf. These components and what they contribute to your stack are discussed in detail in our architectural guide . Briefly, they include: Your Git repo (either GitHub or GitLab) that holds your application code. AWS CodeBuild to build and deploy your project as a Docker container. An Amazon Elastic Container Repository (ECR) repo for storing your Docker container's versions. An Amazon Elastic Container Service (ECS) cluster for hosting your running Docker containers. Amazon API Gateway for routing traffic to your application's exposed endpoints. Amazon CloudWatch for monitoring performance and storing application logs. An optional database for storing application data. Additionally, you can see and configure several additional options: The AWS pricing breakdown gives you a sense of what you'll per month for your stack in its current configuration. You can define Environment variables as name-value pairs that will be exposed as environment variables to your running application's Docker container. If a tile has a gear icon in the upper right corner, you can click it to configure advanced options for that tile. Configurable options include the following. Configure Your VPC (optional for serverless) If you choose container architecture, TinyStacks will by default create a new, secure VPC with three public and three isolated subnets. Your application will run into the isolated subnets. You can click the gear icon to opt instead to use your own pre-created VPC from your AWS account. You can use your own VPC that you created or a VPC that TinyStacks created for one of your other stacks. You can also option to turn the isolated subnets into private subnets by adding a NAT Gateway. This will enable applications running in these subnets to make calls out to the public Internet. You should enable this if your application has an external dependency, such as on a publicly hosted Web service. (Note: You will incur an hourly charge for your NAT Gateway as well as data transfer charges for every 1GB of data that flows to the Internet or to compute capacity in a different Availability Zone. See the AWS NAT Gateway Pricing page for more details.) Note that serverless applications, by default, do not run in a VPC. You can, however, opt to run them in a VPC that's already defined in your AWS account. Add a Database Click Enable to create an Amazon RDS Postgres database for your account. Use the settings option to select an existing database instead and to configure other database-related options. Amazon ECS (container architecture only) Configure build and scale settings for your Amazon ECS cluster, such as the size of the Amazon EC2 instances used in your cluster. (If you're not familiar with these settings, you can leave them as the default for now.) AWS Lambda (serverless architecture only) Configure scale settings for your AWS Lambda function, including the memory available to your application and the runtime timeout setting. (If you're not familiar with these settings, you can leave them as the default for now.) Front-End Routing Choose between using Amazon API Gateway or Application Load Balancer for front end application routing. You can change any of these options now or change them later, after your original deployment. Your stack will also build as is, without any additional configuration. For now, start the creation of your first stack by clicking Build . Testing your stack Your application's environment will take a few minutes to build. We'll present detailed status updates in the upper-right corner of this as we build out your stack. Once it's done, you'll be taken to the Stacks page on your TinyStacks account, where you can see your running stack listed. You can also set the minimum and maximum number of instances of your container that ECS should run in response to scale out and scale in events. (For more information, see Autoscaling .) Click on your stack's name to navigate to the Stack Details page. This page shows your stack and all of the stages you've defined. The initial stack creation process creates a single stage named dev . You can use the Add stage button to add more stages at any time. For now, let's tets out the dev stack and ensure it's working. On the lower right corner of the dev box, click Copy endpoint . Then, open a new tab in your Web browser, paste in the copied URL, and press Enter . You should navigate to the Express application's ping page, which should return a string reading Healthy connection . You can also test other endpoints of the API as defined by the Express app README. For example, to add an item to the application's memory, you can use the following command on Linux systems. Be sure to replace my-domain-name with the domain portion of your application's URL, which will be of the form unique-id.execute-api.us-east-1.amazonaws.com : curl -H \"Content-Type: application/json\" -X PUT -d '{\"title\":\"my title\", \"content\" : \"my content\"}' \"https://my-domain-name/item\" On Windows Powershell, use the following command: $item = @{ title=\"my title\" content=\"my content\" } $json = $item | ConvertTo-Json $response = Invoke-WebRequest 'https://my-domain-name/local-item' -Method Put -Body $json -ContentType 'application/json' -UseBasicParsing If everything is configured correctly, you'll receive a 200 HTTP response status code for the operation. Congratulations - you've deployed your first stack! Push your first commit Your stack is configured to deploy any changes you make to your code automatically to your dev stage. To see this in action, let's make a small change to your code. Navigate to your Git repo. You can do this easily by clicking on the GitHub or GitLab icon on your stack's tile on the Stack Details page. In the repository, navigate to the file src/server.ts . Copy the /ping method and create a new method called /pong . Return a custom message from the method in the response: app.get(\"/ping\", (req, res) => { res.status(200).send(\"Setting a custom message\"); }); Next, commit your change. In the GitHub UI, you can do this by clicking the Commit changes button at the bottom of the page you're editing. Return to TinyStacks and to the Stack Details page for your stack. Within a couple of minutes, you should see that TinyStacks has picked up the change to your application and is applying the changes to your stack. The first tile will spin while it is building your changes into a new Docker container. Then, you will see the dev stage update as the change is deployed. Wait until the change has propagated to your dev stage. Then, test the /pong URL with cUrl to see your new endpoint. On Linux, run the following command: curl https://my-domain-name/pong On Windows Powershell, use: curl https://my-domain-name/pong -UseBasicParsing","title":"Create your first stack"},{"location":"create-stack/#prerequisites","text":"Before you begin this guide, make sure you have set up your AWS and Git service connections as outlined in Configure AWS and Git service connections .","title":"Prerequisites"},{"location":"create-stack/#select-a-starter-project","text":"Next, you'll be prompted to select a project. You can either select a TinyStacks starter project or use your own project. For this guide, we'll be using a starter project, so go ahead and click the Starter projects button. The TinyStacks starter projects are simple Create/Read/Update/Delete (CRUD) database applications written in the application framework of your choice. You can click the name of any starter project in the window below to see its repository on GitHub and check out its README, which gives a detailed description of the app's code and deployment architecture. Once you select a project, TinyStacks will copy its code into your Git account, where you can use it as a basis for further development. For this guide, let's create an Express project . Next to Express in the window above, click Deploy . On the next screen, you'll be prompted to give your project a name. Choose a simple, short name that's 20 characters or less. (The name will be used as a prefix for some of the resources in your AWS account, so we want to keep it short to avoid going over naming length limits in AWS.)","title":"Select a starter project"},{"location":"create-stack/#choosing-serverless-or-container-deployment","text":"You have one more step to go and then you're ready to launch your stack! After clicking Deploy , you'll see the screen below. Here, you need to select whether to launch your application using either serverless or container architecture. To keep costs low for this initial walkthrough, select Serverless . (For more information on which architecture to choose for production applications, see Architecture and Serverless .)","title":"Choosing serverless or container deployment"},{"location":"create-stack/#further-customizing-your-deployment","text":"This screen visualizes all of the AWS resources that TinyStacks will use or create on your behalf. These components and what they contribute to your stack are discussed in detail in our architectural guide . Briefly, they include: Your Git repo (either GitHub or GitLab) that holds your application code. AWS CodeBuild to build and deploy your project as a Docker container. An Amazon Elastic Container Repository (ECR) repo for storing your Docker container's versions. An Amazon Elastic Container Service (ECS) cluster for hosting your running Docker containers. Amazon API Gateway for routing traffic to your application's exposed endpoints. Amazon CloudWatch for monitoring performance and storing application logs. An optional database for storing application data. Additionally, you can see and configure several additional options: The AWS pricing breakdown gives you a sense of what you'll per month for your stack in its current configuration. You can define Environment variables as name-value pairs that will be exposed as environment variables to your running application's Docker container. If a tile has a gear icon in the upper right corner, you can click it to configure advanced options for that tile. Configurable options include the following.","title":"Further customizing your deployment"},{"location":"create-stack/#configure-your-vpc-optional-for-serverless","text":"If you choose container architecture, TinyStacks will by default create a new, secure VPC with three public and three isolated subnets. Your application will run into the isolated subnets. You can click the gear icon to opt instead to use your own pre-created VPC from your AWS account. You can use your own VPC that you created or a VPC that TinyStacks created for one of your other stacks. You can also option to turn the isolated subnets into private subnets by adding a NAT Gateway. This will enable applications running in these subnets to make calls out to the public Internet. You should enable this if your application has an external dependency, such as on a publicly hosted Web service. (Note: You will incur an hourly charge for your NAT Gateway as well as data transfer charges for every 1GB of data that flows to the Internet or to compute capacity in a different Availability Zone. See the AWS NAT Gateway Pricing page for more details.) Note that serverless applications, by default, do not run in a VPC. You can, however, opt to run them in a VPC that's already defined in your AWS account.","title":"Configure Your VPC (optional for serverless)"},{"location":"create-stack/#add-a-database","text":"Click Enable to create an Amazon RDS Postgres database for your account. Use the settings option to select an existing database instead and to configure other database-related options.","title":"Add a Database"},{"location":"create-stack/#amazon-ecs-container-architecture-only","text":"Configure build and scale settings for your Amazon ECS cluster, such as the size of the Amazon EC2 instances used in your cluster. (If you're not familiar with these settings, you can leave them as the default for now.)","title":"Amazon ECS (container architecture only)"},{"location":"create-stack/#aws-lambda-serverless-architecture-only","text":"Configure scale settings for your AWS Lambda function, including the memory available to your application and the runtime timeout setting. (If you're not familiar with these settings, you can leave them as the default for now.)","title":"AWS Lambda (serverless architecture only)"},{"location":"create-stack/#front-end-routing","text":"Choose between using Amazon API Gateway or Application Load Balancer for front end application routing. You can change any of these options now or change them later, after your original deployment. Your stack will also build as is, without any additional configuration. For now, start the creation of your first stack by clicking Build .","title":"Front-End Routing"},{"location":"create-stack/#testing-your-stack","text":"Your application's environment will take a few minutes to build. We'll present detailed status updates in the upper-right corner of this as we build out your stack. Once it's done, you'll be taken to the Stacks page on your TinyStacks account, where you can see your running stack listed. You can also set the minimum and maximum number of instances of your container that ECS should run in response to scale out and scale in events. (For more information, see Autoscaling .) Click on your stack's name to navigate to the Stack Details page. This page shows your stack and all of the stages you've defined. The initial stack creation process creates a single stage named dev . You can use the Add stage button to add more stages at any time. For now, let's tets out the dev stack and ensure it's working. On the lower right corner of the dev box, click Copy endpoint . Then, open a new tab in your Web browser, paste in the copied URL, and press Enter . You should navigate to the Express application's ping page, which should return a string reading Healthy connection . You can also test other endpoints of the API as defined by the Express app README. For example, to add an item to the application's memory, you can use the following command on Linux systems. Be sure to replace my-domain-name with the domain portion of your application's URL, which will be of the form unique-id.execute-api.us-east-1.amazonaws.com : curl -H \"Content-Type: application/json\" -X PUT -d '{\"title\":\"my title\", \"content\" : \"my content\"}' \"https://my-domain-name/item\" On Windows Powershell, use the following command: $item = @{ title=\"my title\" content=\"my content\" } $json = $item | ConvertTo-Json $response = Invoke-WebRequest 'https://my-domain-name/local-item' -Method Put -Body $json -ContentType 'application/json' -UseBasicParsing If everything is configured correctly, you'll receive a 200 HTTP response status code for the operation. Congratulations - you've deployed your first stack!","title":"Testing your stack"},{"location":"create-stack/#push-your-first-commit","text":"Your stack is configured to deploy any changes you make to your code automatically to your dev stage. To see this in action, let's make a small change to your code. Navigate to your Git repo. You can do this easily by clicking on the GitHub or GitLab icon on your stack's tile on the Stack Details page. In the repository, navigate to the file src/server.ts . Copy the /ping method and create a new method called /pong . Return a custom message from the method in the response: app.get(\"/ping\", (req, res) => { res.status(200).send(\"Setting a custom message\"); }); Next, commit your change. In the GitHub UI, you can do this by clicking the Commit changes button at the bottom of the page you're editing. Return to TinyStacks and to the Stack Details page for your stack. Within a couple of minutes, you should see that TinyStacks has picked up the change to your application and is applying the changes to your stack. The first tile will spin while it is building your changes into a new Docker container. Then, you will see the dev stage update as the change is deployed. Wait until the change has propagated to your dev stage. Then, test the /pong URL with cUrl to see your new endpoint. On Linux, run the following command: curl https://my-domain-name/pong On Windows Powershell, use: curl https://my-domain-name/pong -UseBasicParsing","title":"Push your first commit"},{"location":"customizing/","text":"TinyStacks contains many of the capabilities you'll need to run your application out of the box. But we also understand that some customization will always be required. In a few cases, you may need to make significant additions to your stacks for them to function properly. TinyStacks supports extending and building upon your stack's architecture and operation in a number of ways. Connecting to other AWS resources from your application As you grow your application, you will likely want to incorporate access to other AWS services. Common examples include DynamoDB for NoSQL data storage and Amazon S3 for storing blobs and other large data. Currently, there are two ways to do this on TinyStacks. Note that both operations require knowledge of AWS Identity and Access Management (IAM). Pass AWS credentials as runtime variables You can create an AWS access key and secret key and pass these variables as runtime variables as described above in passing runtime variables to a stage. Note that, if you do this, anyone with access to your TinyStacks account has access to these variables, and thus to your underlying AWS account. Please proceed with caution. Modify the IAM role for your application Your Amazon ECS tasks all run with an IAM task role that determines their AWS permissions. The task role name is a combination of the stack name, the stage name, and the phrase \"TaskRoleDefaultPolicy\". For example, below is the IAM role for the dev stage for a stack named test-django2 . By default, this role only has permission to push events to your Amazon CloudWatch Logs log group. You can modify it with a valid IAm policy to grant it access to additional AWS resources. Custom post-deploy events You may want to take additional actions after a TinyStacks deployment finishes. This may be as simple as notifying your team that a deployment on a specific stage has completed. Or you may even want to run custom code that takes additional actions, such as calling third-party APIs or even standing up additional AWS infrastructure. TinyStacks supports triggering custom build events on your stack via an email notification. You can send a JSON-formatted email to a service such as Amazon Simple Email Service (SNS) , which can further trigger running custom code such as an AWS Lambda function.","title":"Customizing your stacks"},{"location":"customizing/#connecting-to-other-aws-resources-from-your-application","text":"As you grow your application, you will likely want to incorporate access to other AWS services. Common examples include DynamoDB for NoSQL data storage and Amazon S3 for storing blobs and other large data. Currently, there are two ways to do this on TinyStacks. Note that both operations require knowledge of AWS Identity and Access Management (IAM).","title":"Connecting to other AWS resources from your application"},{"location":"customizing/#pass-aws-credentials-as-runtime-variables","text":"You can create an AWS access key and secret key and pass these variables as runtime variables as described above in passing runtime variables to a stage. Note that, if you do this, anyone with access to your TinyStacks account has access to these variables, and thus to your underlying AWS account. Please proceed with caution.","title":"Pass AWS credentials as runtime variables"},{"location":"customizing/#modify-the-iam-role-for-your-application","text":"Your Amazon ECS tasks all run with an IAM task role that determines their AWS permissions. The task role name is a combination of the stack name, the stage name, and the phrase \"TaskRoleDefaultPolicy\". For example, below is the IAM role for the dev stage for a stack named test-django2 . By default, this role only has permission to push events to your Amazon CloudWatch Logs log group. You can modify it with a valid IAm policy to grant it access to additional AWS resources.","title":"Modify the IAM role for your application"},{"location":"customizing/#custom-post-deploy-events","text":"You may want to take additional actions after a TinyStacks deployment finishes. This may be as simple as notifying your team that a deployment on a specific stage has completed. Or you may even want to run custom code that takes additional actions, such as calling third-party APIs or even standing up additional AWS infrastructure. TinyStacks supports triggering custom build events on your stack via an email notification. You can send a JSON-formatted email to a service such as Amazon Simple Email Service (SNS) , which can further trigger running custom code such as an AWS Lambda function.","title":"Custom post-deploy events"},{"location":"database/","text":"TinyStacks supports creating a Postgres database as part of your stack. You can also choose to pass in a database you have already created in your AWS account. Changing your stack's database settings Go to your Stacks page and, for the stack you want to modify, click the gear icon in the lower right corner. This will open the Settings page. In the left hand column, under Stage , select the stage to which you want to add a database. Then, from the same column, select Database . You will see two different screens here depending on whether your stage currently has a database or not. If it has a database, you will see a screen that enables you to change the instance type for your database as well as the amount of disk storage used. If your stage currently has no database, you will see the following screen. You can either add a new Postgres database, or add another existing AWS database to your stack. Note : Saving your changes will trigger a rebuild of your stage. Your application may not be accessible on this stage until the rebuild and redeploy has completed. Connecting to your Postgres database from your application When you have TinyStacks create a Postgres database for you, we push all information about the database - including username and password - into a set of runtime variables. These are exposed to your application as environment variables in your Docker container instances. The variables pushed include: PG_HOST : The IP address PG_PORT : The port on which the Postgres database is running PG_CREDENTIALS_SECRET : The password for connecting to your Postgres database PG_DATABASE : The name of the database on the database host server Your application code should be able to access these values the same as it would any other environment variable (e.g., process.env in Node.js or os.environ.get() in Python ). Using a bastion host By default, your Postgres database is only accessible from the same VPC in which your application runs. However, many teams will want to connect directly to their databases to run queries with tools like MySQL Workbench. To enable these scenarios, you need to create a bastion host . The bastion host is an Amazon EC2 instance that sits in your VPC and creates a secure SSH tunnel so you can connect to your database. The SSH connection requires using public key cryptography, which prevents unauthorized users from connecting to your bastion and attempting to access your database. Creating a bastion host You can create a bastion host when you initially launch your stack and configure your Postgres database. If you didn't add a database to your stack yet, you can navigate to Stack Settings for your stack and add it under Stage settings -> Database . Connecting to your bastion host To connect to your database, navigate to your stack's Stack settings page. Under Stage settings -> Database , you'll see a box labeled Bastion connection information . Here, you can download the private key you'll need to connect to your database. Once downloaded, place your private key somewhere on your computer where you can reference it easily. Make sure you secure the machine on which the private key exists; anyone with access to this key and your bastion's connection information could use this to attempt to gain access to your database. The command displayed in this box will enable you to connect to your bastion via the command line, where you can use command-line Postgres tools to query your database. If you prefer to use a visual tool, like MySQL Workbench, you can configure the tool to connect to your database via an SSH tunnel using your connection information and your SSH private key. Consult your tool's documentation for detailed instructions. Accessing your database's credentials (username and password) You will, of course, still need your database's username and password to connect to it! Your Postgres database username and password are stored securely in your AWS account using AWS Secrets Manager . You can see these secrets by navigating to AWS Secrets Manager in your AWS account. You can identify the correct secret for your application from the AWS Console in two ways: The Secret description of the stack, which will contain the name you gave your stack at stack creation time. The tag aws:cloudformation:stack-name , which will also contain the name of your secret. If you have multiple stages, you will have multiple secrets. You can distinguish them by the name of the secret, which will have the name of the stage pre-pended. In this example, since this is our Postgres database for our dev stage, it starts with the prefix dev . In the AWS Console, you can see the information stored in this secret by expanding the Secret value dropdown. There, you can see all of the information required to connect to your Postgres database. If you need to retrieve these values programmatically, you can do so using the AWS Command Line Interface (CLI) or any of the APIs available for programmatic access. For example, using the AWS CLI, you can retrieve the secret you need by searching for all secrets where the tag aws:cloudformation:stack-name matches your stack name. You can then use the command jq to filter out the return values to get the correct database for your stage. aws secretsmanager list-secrets --region us-east-1 --filters Key=tag-key Values=aws:cloudformation:stack-name Key=tag-value,Values=test-django2 | jq '.[][] | select(.Name|test(\"^dev\"))","title":"Database"},{"location":"database/#changing-your-stacks-database-settings","text":"Go to your Stacks page and, for the stack you want to modify, click the gear icon in the lower right corner. This will open the Settings page. In the left hand column, under Stage , select the stage to which you want to add a database. Then, from the same column, select Database . You will see two different screens here depending on whether your stage currently has a database or not. If it has a database, you will see a screen that enables you to change the instance type for your database as well as the amount of disk storage used. If your stage currently has no database, you will see the following screen. You can either add a new Postgres database, or add another existing AWS database to your stack. Note : Saving your changes will trigger a rebuild of your stage. Your application may not be accessible on this stage until the rebuild and redeploy has completed.","title":"Changing your stack's database settings"},{"location":"database/#connecting-to-your-postgres-database-from-your-application","text":"When you have TinyStacks create a Postgres database for you, we push all information about the database - including username and password - into a set of runtime variables. These are exposed to your application as environment variables in your Docker container instances. The variables pushed include: PG_HOST : The IP address PG_PORT : The port on which the Postgres database is running PG_CREDENTIALS_SECRET : The password for connecting to your Postgres database PG_DATABASE : The name of the database on the database host server Your application code should be able to access these values the same as it would any other environment variable (e.g., process.env in Node.js or os.environ.get() in Python ).","title":"Connecting to your Postgres database from your application"},{"location":"database/#using-a-bastion-host","text":"By default, your Postgres database is only accessible from the same VPC in which your application runs. However, many teams will want to connect directly to their databases to run queries with tools like MySQL Workbench. To enable these scenarios, you need to create a bastion host . The bastion host is an Amazon EC2 instance that sits in your VPC and creates a secure SSH tunnel so you can connect to your database. The SSH connection requires using public key cryptography, which prevents unauthorized users from connecting to your bastion and attempting to access your database.","title":"Using a bastion host"},{"location":"database/#creating-a-bastion-host","text":"You can create a bastion host when you initially launch your stack and configure your Postgres database. If you didn't add a database to your stack yet, you can navigate to Stack Settings for your stack and add it under Stage settings -> Database .","title":"Creating a bastion host"},{"location":"database/#connecting-to-your-bastion-host","text":"To connect to your database, navigate to your stack's Stack settings page. Under Stage settings -> Database , you'll see a box labeled Bastion connection information . Here, you can download the private key you'll need to connect to your database. Once downloaded, place your private key somewhere on your computer where you can reference it easily. Make sure you secure the machine on which the private key exists; anyone with access to this key and your bastion's connection information could use this to attempt to gain access to your database. The command displayed in this box will enable you to connect to your bastion via the command line, where you can use command-line Postgres tools to query your database. If you prefer to use a visual tool, like MySQL Workbench, you can configure the tool to connect to your database via an SSH tunnel using your connection information and your SSH private key. Consult your tool's documentation for detailed instructions.","title":"Connecting to your bastion host"},{"location":"database/#accessing-your-databases-credentials-username-and-password","text":"You will, of course, still need your database's username and password to connect to it! Your Postgres database username and password are stored securely in your AWS account using AWS Secrets Manager . You can see these secrets by navigating to AWS Secrets Manager in your AWS account. You can identify the correct secret for your application from the AWS Console in two ways: The Secret description of the stack, which will contain the name you gave your stack at stack creation time. The tag aws:cloudformation:stack-name , which will also contain the name of your secret. If you have multiple stages, you will have multiple secrets. You can distinguish them by the name of the secret, which will have the name of the stage pre-pended. In this example, since this is our Postgres database for our dev stage, it starts with the prefix dev . In the AWS Console, you can see the information stored in this secret by expanding the Secret value dropdown. There, you can see all of the information required to connect to your Postgres database. If you need to retrieve these values programmatically, you can do so using the AWS Command Line Interface (CLI) or any of the APIs available for programmatic access. For example, using the AWS CLI, you can retrieve the secret you need by searching for all secrets where the tag aws:cloudformation:stack-name matches your stack name. You can then use the command jq to filter out the return values to get the correct database for your stage. aws secretsmanager list-secrets --region us-east-1 --filters Key=tag-key Values=aws:cloudformation:stack-name Key=tag-value,Values=test-django2 | jq '.[][] | select(.Name|test(\"^dev\"))","title":"Accessing your database's credentials (username and password)"},{"location":"domain-names-https/","text":"Using your domain name You can use your own domain name for your application's endpoint. How you set it up depends on whether you use API Gateway or Application Load Balancer (ALB) as your endpoint technology . Application Load Balancer For ALB, you need only create a single DNS record pointing to the ALB's public endpoint. Use the following steps to set up DNS records for a domain whose DNS is managed by AWS. Navigate to Route 53 in your AWS account and selected Hosted Zones . From there, select the entry for your domain name. On the Records page, click Create record . On the Create Record page, under Record type , select SRV . Then, under Value , enter a record in this format: 1 1 80 alb-address The results should look like this: The first two entries are the priority and weight; they're used for distributing traffic across multiple addresses. The third value is your port number; change it if you are serving your service or sidecar over a port other than 80. The last value is your ALB endpoint. You can copy this from the Copy endpoint command from the appropriate stage on your stack. Make sure to remove the prefix (http://) as well as the port number (the colon and everything after). Note that you can also create an A record as an alias to Application Load Balancer as discussed in the official AWS documentation . We recommend using SRV records if you are running multiple images (e.g., sidecars ) in your stack, as SRV allows you to create domain entries that route traffic to different ports on the same host. API Gateway There are a few additional steps for attaching your domain to an API Gateway endpoint. First, you will need to create an Amazon Certificate Manager (ACM) certificate for your domain name. For detailed steps, consult the AWS documentation . Note that, if you wish to use a subdomain to expose your application (e.g., api.mydomain.com), you need to include that subdomain among the list of names covered by the certificate. Next, navigate to API Gateway in the AWS Console and create a custom domain name. In API Gateway, click Custom domain names . Enter your domain name in the dialog box and click Create . On the next page, under Domain details , enter a domain our a subdomain to use (e.g., api.domainname.com). Select the certificate you created earlier in the process as well. You now have a custom domain name in API Gateway. However, you still need to map it to your API. To do this, on the custom domain name's page, click API mappings . Select your API from the API list. Your API Gateway for your deployed stage will contain both the name of your stack and the name of the stack/stage. For example, if your stack is named express-test , the dev stage will have the name express-test-express-test-dev . Leave all other parameters as their default values and click Save . The last step is to create an A record for your domain that maps your domain name to the endpoint you created for your custom domain name. Navigate to Route 53 and, under hosted zones for your domain, click Create record . From there, create an A record and select Alias . Then, select API Gateway and the region your endpoint is in. You should see the endpoint for your custom domain name in API Gateway in the drop-down list. You should now be able to navigate to your domain. Since your domain is hosted in AWS API Gateway, it uses HTTPS by default, so make sure to prefix your domain name with https:// when typing it into a Web browser. HTTPS for your stack Users of API Gateway get HTTPS connectivity by default. You can use your own domain with HTTPS by following the process outlined above for using a custom domain name with API Gateway. For Application Load Balancer, TinyStacks can incorporate your custom domain name and certificate into your stack for end to end SSL termination. Contact us to initiate the process.","title":"Domain names and HTTPS"},{"location":"domain-names-https/#using-your-domain-name","text":"You can use your own domain name for your application's endpoint. How you set it up depends on whether you use API Gateway or Application Load Balancer (ALB) as your endpoint technology .","title":"Using your domain name"},{"location":"domain-names-https/#application-load-balancer","text":"For ALB, you need only create a single DNS record pointing to the ALB's public endpoint. Use the following steps to set up DNS records for a domain whose DNS is managed by AWS. Navigate to Route 53 in your AWS account and selected Hosted Zones . From there, select the entry for your domain name. On the Records page, click Create record . On the Create Record page, under Record type , select SRV . Then, under Value , enter a record in this format: 1 1 80 alb-address The results should look like this: The first two entries are the priority and weight; they're used for distributing traffic across multiple addresses. The third value is your port number; change it if you are serving your service or sidecar over a port other than 80. The last value is your ALB endpoint. You can copy this from the Copy endpoint command from the appropriate stage on your stack. Make sure to remove the prefix (http://) as well as the port number (the colon and everything after). Note that you can also create an A record as an alias to Application Load Balancer as discussed in the official AWS documentation . We recommend using SRV records if you are running multiple images (e.g., sidecars ) in your stack, as SRV allows you to create domain entries that route traffic to different ports on the same host.","title":"Application Load Balancer"},{"location":"domain-names-https/#api-gateway","text":"There are a few additional steps for attaching your domain to an API Gateway endpoint. First, you will need to create an Amazon Certificate Manager (ACM) certificate for your domain name. For detailed steps, consult the AWS documentation . Note that, if you wish to use a subdomain to expose your application (e.g., api.mydomain.com), you need to include that subdomain among the list of names covered by the certificate. Next, navigate to API Gateway in the AWS Console and create a custom domain name. In API Gateway, click Custom domain names . Enter your domain name in the dialog box and click Create . On the next page, under Domain details , enter a domain our a subdomain to use (e.g., api.domainname.com). Select the certificate you created earlier in the process as well. You now have a custom domain name in API Gateway. However, you still need to map it to your API. To do this, on the custom domain name's page, click API mappings . Select your API from the API list. Your API Gateway for your deployed stage will contain both the name of your stack and the name of the stack/stage. For example, if your stack is named express-test , the dev stage will have the name express-test-express-test-dev . Leave all other parameters as their default values and click Save . The last step is to create an A record for your domain that maps your domain name to the endpoint you created for your custom domain name. Navigate to Route 53 and, under hosted zones for your domain, click Create record . From there, create an A record and select Alias . Then, select API Gateway and the region your endpoint is in. You should see the endpoint for your custom domain name in API Gateway in the drop-down list. You should now be able to navigate to your domain. Since your domain is hosted in AWS API Gateway, it uses HTTPS by default, so make sure to prefix your domain name with https:// when typing it into a Web browser.","title":"API Gateway"},{"location":"domain-names-https/#https-for-your-stack","text":"Users of API Gateway get HTTPS connectivity by default. You can use your own domain with HTTPS by following the process outlined above for using a custom domain name with API Gateway. For Application Load Balancer, TinyStacks can incorporate your custom domain name and certificate into your stack for end to end SSL termination. Contact us to initiate the process.","title":"HTTPS for your stack"},{"location":"environment-variables/","text":"TinyStacks supports customizing your deployments through environment variables. We support two types of environment variables: build and runtime. Build environment variables Build environment variables are passed to your build.yml file as part of the container build process. You can use build environment variables to Adding and using build environment variables To add a build environment variable, you must first add it to your stack. On the TinyStacks dashboard, click the gear icon for your stack to go to its Stack Settings page. Under Build variables, you can add a new key-value pair by clicking the + button. After you've defined the build environment variable here, you can pass it into your build.yml file to customize the build. Example: Defining a build in a monorepo For example, say that you wanted to use a monorepo architecture, in which you deploy multiple services from a single Git repository. By default, TinyStacks looks for your Dockerfile at the root of the repository. To use a different path, you could define it as a build environment variable in TinyStacks: Then, simply use this variable as the path to your Dockerfile by editing the build.yml to use the new variable: - docker build -t builtimage . -f $DOCKERFILE_PATH (Note: We recommend adding this after adding the variable to your stack in the TinyStacks dashboard. If you add it before, the check-in will trigger a deployment in TinyStacks, which will fail as the variable is undefined.) By defining a different path for each stack, you can use the same build.yml and release.yml files in a repo to deploy multiple services. Runtime environment variables Runtime variables are set on your Docker container images. They are copied over with each container image as a new image is run in response to an application scale event. Runtime variables are essential to adding additional functionality to your application. It's likely that your application already needs various runtime variables set in order to run. For example, you may need to supply credentials to a data store, such as DynamoDB, or information on how to connect to other microservices on which the application depends. Each stage of your application will likely need its own custom runtime variables, as the configuration and the resources to which your application connects will vary by stage. TinyStacks makes it easy to set runtime variables on your application that are unique to each stage. This enables you to have different configuration settings for your application depending on whether it's in dev, test, or production. Adding or changing runtime variables per stage Adding a new runtime environment variable is a two-step process. To add or change runtime variables on your stack, go to your Stacks page . For the stack you want to modify, click the gear icon in the lower right corner. In the Stage dropdown on the left hand navigation menu, make sure you select the stage of your stack that you want to modify. Then, to see your runtime variables, click Runtime variables . Note : If you had TinyStacks create a database for your stage, you may already see some variables for connecting to your database defined here. You can add any key-value pair you wish to your stage. When done, click Save runtime variables to save your changes. Next, you need to add support for passing the new variable into your Dockerfile. To do this, edit your Dockerfile to declare a variable of the same name with the Docker ARG command: ARG FOO Finally, edit your build.yml file to pass this parameter to your container at build time using the --build-arg argument to the docker build command: - docker build -t builtimage . --build-arg FOO=$FOO Once the rebuild and redeploy is complete, the key-value pairs you defined will be exposed as environment variables on your Docker container. Your application code should be able to access these values the same as it would any other environment variable (e.g., process.env in Node.js or os.environ.get() in Python ).","title":"Environment variables"},{"location":"environment-variables/#build-environment-variables","text":"Build environment variables are passed to your build.yml file as part of the container build process. You can use build environment variables to","title":"Build environment variables"},{"location":"environment-variables/#adding-and-using-build-environment-variables","text":"To add a build environment variable, you must first add it to your stack. On the TinyStacks dashboard, click the gear icon for your stack to go to its Stack Settings page. Under Build variables, you can add a new key-value pair by clicking the + button. After you've defined the build environment variable here, you can pass it into your build.yml file to customize the build.","title":"Adding and using build environment variables"},{"location":"environment-variables/#example-defining-a-build-in-a-monorepo","text":"For example, say that you wanted to use a monorepo architecture, in which you deploy multiple services from a single Git repository. By default, TinyStacks looks for your Dockerfile at the root of the repository. To use a different path, you could define it as a build environment variable in TinyStacks: Then, simply use this variable as the path to your Dockerfile by editing the build.yml to use the new variable: - docker build -t builtimage . -f $DOCKERFILE_PATH (Note: We recommend adding this after adding the variable to your stack in the TinyStacks dashboard. If you add it before, the check-in will trigger a deployment in TinyStacks, which will fail as the variable is undefined.) By defining a different path for each stack, you can use the same build.yml and release.yml files in a repo to deploy multiple services.","title":"Example: Defining a build in a monorepo"},{"location":"environment-variables/#runtime-environment-variables","text":"Runtime variables are set on your Docker container images. They are copied over with each container image as a new image is run in response to an application scale event. Runtime variables are essential to adding additional functionality to your application. It's likely that your application already needs various runtime variables set in order to run. For example, you may need to supply credentials to a data store, such as DynamoDB, or information on how to connect to other microservices on which the application depends. Each stage of your application will likely need its own custom runtime variables, as the configuration and the resources to which your application connects will vary by stage. TinyStacks makes it easy to set runtime variables on your application that are unique to each stage. This enables you to have different configuration settings for your application depending on whether it's in dev, test, or production.","title":"Runtime environment variables"},{"location":"environment-variables/#adding-or-changing-runtime-variables-per-stage","text":"Adding a new runtime environment variable is a two-step process. To add or change runtime variables on your stack, go to your Stacks page . For the stack you want to modify, click the gear icon in the lower right corner. In the Stage dropdown on the left hand navigation menu, make sure you select the stage of your stack that you want to modify. Then, to see your runtime variables, click Runtime variables . Note : If you had TinyStacks create a database for your stage, you may already see some variables for connecting to your database defined here. You can add any key-value pair you wish to your stage. When done, click Save runtime variables to save your changes. Next, you need to add support for passing the new variable into your Dockerfile. To do this, edit your Dockerfile to declare a variable of the same name with the Docker ARG command: ARG FOO Finally, edit your build.yml file to pass this parameter to your container at build time using the --build-arg argument to the docker build command: - docker build -t builtimage . --build-arg FOO=$FOO Once the rebuild and redeploy is complete, the key-value pairs you defined will be exposed as environment variables on your Docker container. Your application code should be able to access these values the same as it would any other environment variable (e.g., process.env in Node.js or os.environ.get() in Python ).","title":"Adding or changing runtime variables per stage"},{"location":"existing/","text":"In our guide to creating your first stack , we showed you how to launch one of our sample applications as a fully scalable deployment on the cloud. But it's just as easy to package and deploy an existing application! Prerequisites Before you begin this guide, make sure you have set up your AWS and Git service connections as outlined in Configure AWS and Git service connections . Create a new stack After you configure your connections, you'll be prompted to select a project. You can either select a TinyStacks starter project or use your own project. For this guide, we'll be using your own project. Select My projects . You should see a list of all of the repositories available in your Git account. Find the repository whose code you are going to deploy and click Prepare to deploy . You should now be on the Prepare your repository page. Check the deployment template files into your Git repository When you choose to deploy code from an existing repository, TinyStacks checks your repo to see if it contains three files: A Dockerfile . This file contains the instructions for preparing your Docker container, which will run your application code. Your container will need to contain your application framework (Express, Flask, Django, etc.), as well as any configuration files and environment variables necessary to run your application. A build.yml file, which AWS CodeDeploy will use to create the latest version of your Docker container and store it in an Amazon Elastic Code Repository (ECR) repo in your AWS account. A release.yml file, which AWS CodeDeploy will use to run your Docker container on an Amazon Elastic Container Service (ECS) cluster hosted in your AWS account. For more details on the elements of a TinyStacks deployment, see our architecture page . Note : If your app is already Dockerized (has a Dockerfile), you do not need to replace it with our Dockerfile. We only provide a Dockerfile for the benefit of teams who have no yet containerized their applications. Since your Docker container must contain the framework required by your application, it's important that you click the Framework dropdown and select whatever framework your application uses. This will ensure that the Dockerfile that you download can successfully run your application. Once you've selected the correct framework, click download all files at the bottom of the page to download all three files as a zip file. Unzip this file and check in all of the missing files for your application to the root of your repository. The root directory of your repo should look something like the example below after you are done: ( Note : This example is for an Express app - your project may have different files if it is using a different framework. The important thing is that these three files are checked into the root of your repository.) Once you are done, click Next . Name your project and select the deployment branch Next, give your project a name. Since this name will be used as a prefix for many of your AWS resources, keep it to 20 characters or less. You will also need to select a deployment branch. This is the branch of your Git repository on which new check ins will trigger a new deployment. There are two other settings on this page you can optionally configure: Port : If the application in your Docker container is using a port other than the default (80) to serve traffic, enter it here. Custom health check : AWS services such as Application Load Balancer use health checks to determine if an instance of your application is running correctly. Instances that fail a health check are destroyed and replaced with healthy instances. By default, Tinystacks uses the endpoint /ping to check application health. If you use a different endpoint, specify it here. When you're done, click Next . Choosing serverless or container deployment You have one more step to go and then you're ready to launch your stack! After clicking Deploy , you'll see the screen below. Here, you need to select whether to launch your application using either serverless or container architecture. To keep costs low for this initial walkthrough, select Serverless . (For more information on which architecture to choose for production applications, see (Architecture)[architecture.md] and (Serverless)[serverless.md].) Further customizing your deployment This screen visualizes all of the AWS resources that TinyStacks will use or create on your behalf. These components and what they contribute to your stack are discussed in detail in our architectural guide . Briefly, they include: Your Git repo (either GitHub or GitLab) that holds your application code. AWS CodeBuild to build and deploy your project as a Docker container. An Amazon Elastic Container Repository (ECR) repo for storing your Docker container's versions. An Amazon Elastic Container Service (ECS) cluster for hosting your running Docker containers. Amazon API Gateway for routing traffic to your application's exposed endpoints. Amazon CloudWatch for monitoring performance and storing application logs. An optional database for storing application data. Additionally, you can see and configure several additional options: The AWS pricing breakdown gives you a sense of what you'll per month for your stack in its current configuration. You can define Environment variables as name-value pairs that will be exposed as environment variables to your running application's Docker container. If a tile has a gear icon in the upper right corner, you can click it to configure advanced options for that tile. Configurable options include the following. Configure Your VPC (optional for serverless) If you choose container architecture, TinyStacks will by default create a new, secure VPC with three public and three isolated subnets. Your application will run into the isolated subnets. You can click the gear icon to opt instead to use your own pre-created VPC from your AWS account. You can use your own VPC that you created or a VPC that TinyStacks created for one of your other stacks. You can also option to turn the isolated subnets into private subnets by adding a NAT Gateway. This will enable applications running in these subnets to make calls out to the public Internet. You should enable this if your application has an external dependency, such as on a publicly hosted Web service. (Note: You will incur an hourly charge for your NAT Gateway as well as data transfer charges for every 1GB of data that flows to the Internet or to compute capacity in a different Availability Zone. See the AWS NAT Gateway Pricing page for more details.) Note that serverless applications, by default, do not run in a VPC. You can, however, opt to run them in a VPC that's already defined in your AWS account. Add a Database Click Enable to create an Amazon RDS Postgres database for your account. Use the settings option to select an existing database instead and to configure other database-related options. Amazon ECS (container architecture only) Configure build and scale settings for your Amazon ECS cluster, such as the size of the Amazon EC2 instances used in your cluster. (If you're not familiar with these settings, you can leave them as the default for now.) AWS Lambda (serverless architecture only) Configure scale settings for your AWS Lambda function, including the memory available to your application and the runtime timeout setting. (If you're not familiar with these settings, you can leave them as the default for now.) Front-End Routing Choose between using Amazon API Gateway or Application Load Balancer for front end application routing. You can change any of these options now or change them later, after your original deployment. Your stack will also build as is, without any additional configuration. For now, start the creation of your first stack by clicking Build . Testing your stack Once you're ready, click the Build button to build your Docker container and deploy it to AWS. Your application's environment will take a few minutes to build. We'll present detailed status updates in the upper-right corner of this as we build out your stack. Once it's done, you'll be taken to the Stacks page on your TinyStacks account, where you can see your running stack listed. Click on your stack's name to navigate to the Stack Details page. This page shows your stack and all of the stages you've defined. The initial stack creation process creates a single stage named dev . You can use the Add stage button to add more stages at any time. For now, let's tets out the dev stack and ensure it's working. On the lower right corner of the dev box, click Copy endpoint . You can use this base URL to access a valid page or REST API call in your application. If everything is configured correctly, you should see your application return an appropriate response to your request. Properly containerizing your application By following the steps above, your application should deploy and run in the cloud. However, you may notice some issues running your code if you haven't prepared your application to run in a container. As explained in our architectural overview , a Docker container is a virtualized operating system that contains all of the executable files, scripts, shared libraries, configuration files and other dependencies required for your application to run. In order to scale your application to handle upwards of millions of requests, your AWS account will need to run multiple copies of this container across multiple virtual machines. If your application has been designed as a monolithic application, it may make certain assumptions about its runtime environment that won't hold true after your application is containerized. If you are seeing errors or strange behavior in your application, here are a few things to consider. Filesystem storage Each running Docker container instance has its own virtualized filesystem. Data written to this filesystem in one running container instance won't be visible to other running instances. Ensure that any data that all container instances must access is written to a shared location - e.g., a database, a cloud storage service like Amazon S3, or an in-memory cache server like Amazon ElastiCache . Configuration Each running container instance will need its own copies of whatever configuration files your application might need. If you need to update any of these configuration files, you will need either to push changes through Git, or store the configuration in a location that all of your running container instances can access dynamically (such as Amazon S3).","title":"Deploy an existing application"},{"location":"existing/#prerequisites","text":"Before you begin this guide, make sure you have set up your AWS and Git service connections as outlined in Configure AWS and Git service connections .","title":"Prerequisites"},{"location":"existing/#create-a-new-stack","text":"After you configure your connections, you'll be prompted to select a project. You can either select a TinyStacks starter project or use your own project. For this guide, we'll be using your own project. Select My projects . You should see a list of all of the repositories available in your Git account. Find the repository whose code you are going to deploy and click Prepare to deploy . You should now be on the Prepare your repository page.","title":"Create a new stack"},{"location":"existing/#check-the-deployment-template-files-into-your-git-repository","text":"When you choose to deploy code from an existing repository, TinyStacks checks your repo to see if it contains three files: A Dockerfile . This file contains the instructions for preparing your Docker container, which will run your application code. Your container will need to contain your application framework (Express, Flask, Django, etc.), as well as any configuration files and environment variables necessary to run your application. A build.yml file, which AWS CodeDeploy will use to create the latest version of your Docker container and store it in an Amazon Elastic Code Repository (ECR) repo in your AWS account. A release.yml file, which AWS CodeDeploy will use to run your Docker container on an Amazon Elastic Container Service (ECS) cluster hosted in your AWS account. For more details on the elements of a TinyStacks deployment, see our architecture page . Note : If your app is already Dockerized (has a Dockerfile), you do not need to replace it with our Dockerfile. We only provide a Dockerfile for the benefit of teams who have no yet containerized their applications. Since your Docker container must contain the framework required by your application, it's important that you click the Framework dropdown and select whatever framework your application uses. This will ensure that the Dockerfile that you download can successfully run your application. Once you've selected the correct framework, click download all files at the bottom of the page to download all three files as a zip file. Unzip this file and check in all of the missing files for your application to the root of your repository. The root directory of your repo should look something like the example below after you are done: ( Note : This example is for an Express app - your project may have different files if it is using a different framework. The important thing is that these three files are checked into the root of your repository.) Once you are done, click Next .","title":"Check the deployment template files into your Git repository"},{"location":"existing/#name-your-project-and-select-the-deployment-branch","text":"Next, give your project a name. Since this name will be used as a prefix for many of your AWS resources, keep it to 20 characters or less. You will also need to select a deployment branch. This is the branch of your Git repository on which new check ins will trigger a new deployment. There are two other settings on this page you can optionally configure: Port : If the application in your Docker container is using a port other than the default (80) to serve traffic, enter it here. Custom health check : AWS services such as Application Load Balancer use health checks to determine if an instance of your application is running correctly. Instances that fail a health check are destroyed and replaced with healthy instances. By default, Tinystacks uses the endpoint /ping to check application health. If you use a different endpoint, specify it here. When you're done, click Next .","title":"Name your project and select the deployment branch"},{"location":"existing/#choosing-serverless-or-container-deployment","text":"You have one more step to go and then you're ready to launch your stack! After clicking Deploy , you'll see the screen below. Here, you need to select whether to launch your application using either serverless or container architecture. To keep costs low for this initial walkthrough, select Serverless . (For more information on which architecture to choose for production applications, see (Architecture)[architecture.md] and (Serverless)[serverless.md].)","title":"Choosing serverless or container deployment"},{"location":"existing/#further-customizing-your-deployment","text":"This screen visualizes all of the AWS resources that TinyStacks will use or create on your behalf. These components and what they contribute to your stack are discussed in detail in our architectural guide . Briefly, they include: Your Git repo (either GitHub or GitLab) that holds your application code. AWS CodeBuild to build and deploy your project as a Docker container. An Amazon Elastic Container Repository (ECR) repo for storing your Docker container's versions. An Amazon Elastic Container Service (ECS) cluster for hosting your running Docker containers. Amazon API Gateway for routing traffic to your application's exposed endpoints. Amazon CloudWatch for monitoring performance and storing application logs. An optional database for storing application data. Additionally, you can see and configure several additional options: The AWS pricing breakdown gives you a sense of what you'll per month for your stack in its current configuration. You can define Environment variables as name-value pairs that will be exposed as environment variables to your running application's Docker container. If a tile has a gear icon in the upper right corner, you can click it to configure advanced options for that tile. Configurable options include the following.","title":"Further customizing your deployment"},{"location":"existing/#configure-your-vpc-optional-for-serverless","text":"If you choose container architecture, TinyStacks will by default create a new, secure VPC with three public and three isolated subnets. Your application will run into the isolated subnets. You can click the gear icon to opt instead to use your own pre-created VPC from your AWS account. You can use your own VPC that you created or a VPC that TinyStacks created for one of your other stacks. You can also option to turn the isolated subnets into private subnets by adding a NAT Gateway. This will enable applications running in these subnets to make calls out to the public Internet. You should enable this if your application has an external dependency, such as on a publicly hosted Web service. (Note: You will incur an hourly charge for your NAT Gateway as well as data transfer charges for every 1GB of data that flows to the Internet or to compute capacity in a different Availability Zone. See the AWS NAT Gateway Pricing page for more details.) Note that serverless applications, by default, do not run in a VPC. You can, however, opt to run them in a VPC that's already defined in your AWS account.","title":"Configure Your VPC (optional for serverless)"},{"location":"existing/#add-a-database","text":"Click Enable to create an Amazon RDS Postgres database for your account. Use the settings option to select an existing database instead and to configure other database-related options.","title":"Add a Database"},{"location":"existing/#amazon-ecs-container-architecture-only","text":"Configure build and scale settings for your Amazon ECS cluster, such as the size of the Amazon EC2 instances used in your cluster. (If you're not familiar with these settings, you can leave them as the default for now.)","title":"Amazon ECS (container architecture only)"},{"location":"existing/#aws-lambda-serverless-architecture-only","text":"Configure scale settings for your AWS Lambda function, including the memory available to your application and the runtime timeout setting. (If you're not familiar with these settings, you can leave them as the default for now.)","title":"AWS Lambda (serverless architecture only)"},{"location":"existing/#front-end-routing","text":"Choose between using Amazon API Gateway or Application Load Balancer for front end application routing. You can change any of these options now or change them later, after your original deployment. Your stack will also build as is, without any additional configuration. For now, start the creation of your first stack by clicking Build .","title":"Front-End Routing"},{"location":"existing/#testing-your-stack","text":"Once you're ready, click the Build button to build your Docker container and deploy it to AWS. Your application's environment will take a few minutes to build. We'll present detailed status updates in the upper-right corner of this as we build out your stack. Once it's done, you'll be taken to the Stacks page on your TinyStacks account, where you can see your running stack listed. Click on your stack's name to navigate to the Stack Details page. This page shows your stack and all of the stages you've defined. The initial stack creation process creates a single stage named dev . You can use the Add stage button to add more stages at any time. For now, let's tets out the dev stack and ensure it's working. On the lower right corner of the dev box, click Copy endpoint . You can use this base URL to access a valid page or REST API call in your application. If everything is configured correctly, you should see your application return an appropriate response to your request.","title":"Testing your stack"},{"location":"existing/#properly-containerizing-your-application","text":"By following the steps above, your application should deploy and run in the cloud. However, you may notice some issues running your code if you haven't prepared your application to run in a container. As explained in our architectural overview , a Docker container is a virtualized operating system that contains all of the executable files, scripts, shared libraries, configuration files and other dependencies required for your application to run. In order to scale your application to handle upwards of millions of requests, your AWS account will need to run multiple copies of this container across multiple virtual machines. If your application has been designed as a monolithic application, it may make certain assumptions about its runtime environment that won't hold true after your application is containerized. If you are seeing errors or strange behavior in your application, here are a few things to consider.","title":"Properly containerizing your application"},{"location":"existing/#filesystem-storage","text":"Each running Docker container instance has its own virtualized filesystem. Data written to this filesystem in one running container instance won't be visible to other running instances. Ensure that any data that all container instances must access is written to a shared location - e.g., a database, a cloud storage service like Amazon S3, or an in-memory cache server like Amazon ElastiCache .","title":"Filesystem storage"},{"location":"existing/#configuration","text":"Each running container instance will need its own copies of whatever configuration files your application might need. If you need to update any of these configuration files, you will need either to push changes through Git, or store the configuration in a location that all of your running container instances can access dynamically (such as Amazon S3).","title":"Configuration"},{"location":"iam-policy/","text":"TinyStacks uses the following IAM policy to authorize access to your AWS account. Parameters: TinyStacksToken: Description: DO NOT CHANGE OR SHARE THIS. This is a temporary TinyStacks token unique to your user Type: String TinyStacksUserId: Description: DO NOT CHANGE OR SHARE THIS. This is your unique user id, it's necessary for deleting this role Type: String Resources: IdentityAndSecretManagementPolicy: Type: \"AWS::IAM::ManagedPolicy\" Properties: ManagedPolicyName: !Sub \"${AWS::StackName}-secret-and-identity-management-policy\" PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \"iam:List*\" - \"iam:Get*\" - \"iam:*Tag*\" - \"iam:Untag*\" - \"iam:CreateInstanceProfile\" - \"iam:DeleteInstanceProfile\" - \"iam:AddRoleToInstanceProfile\" - \"iam:RemoveRoleFromInstanceProfile\" - \"iam:CreatePolicy*\" - \"iam:DeletePolicy*\" - \"iam:CreateRole\" - \"iam:UpdateRole\" - \"iam:PassRole\" - \"iam:DeleteRole\" - \"iam:PutRolePolicy\" - \"iam:DeleteRolePolicy\" - \"iam:AttachRolePolicy\" - \"iam:UpdateAssumeRolePolicy\" - \"iam:DetachRolePolicy\" - \"iam:CreateServiceLinkedRole\" - \"iam:DeleteServiceLinkedRole\" - \"iam:GetServiceLinkedRoleDeletionStatus\" - \"kms:*Tag*\" - \"kms:Decrypt\" - \"secretsmanager:*Tag*\" - \"secretsmanager:CreateSecret\" - \"secretsmanager:GetSecretValue\" - \"secretsmanager:PutSecretValue\" - \"secretsmanager:DeleteSecret\" - \"secretsmanager:GetRandomPassword\" - \"ssm:PutParameter\" - \"ssm:GetParameters\" - \"ssm:DeleteParameter\" - \"ssm:*Tag*\" Resource: \"*\" Role: Type: \"AWS::IAM::Role\" Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: AWS: arn:aws:iam::849087520365:role/tinystacks-service-role Action: - sts:AssumeRole ManagedPolicyArns: - !Ref IdentityAndSecretManagementPolicy Policies: - PolicyName: AllowAssumingCdkRoles PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \"sts:AssumeRole\" Resource: - !Sub \"arn:aws:iam::${AWS::AccountId}:role/cdk*\" - PolicyName: CdkBucketPolicy PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \"s3:*\" Resource: - \"arn:aws:s3:::cdk*\" - PolicyName: CdkSsmParameterPolicy PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \"ssm:GetParameter\" Resource: - \"arn:aws:ssm::*:parameter/cdk-bootstrap/*\" ScalingPolicy: Type: \"AWS::IAM::ManagedPolicy\" Properties: ManagedPolicyName: !Sub \"${AWS::StackName}-scaling-policy\" PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \"autoscaling:Describe*\" - \"autoscaling:*Tag*\" - \"autoscaling:CreateScalingPlan\" - \"autoscaling:GetScalingPlanResourceForecastData\" - \"autoscaling:UpdateScalingPlan\" - \"autoscaling:DeleteScalingPlan\" - \"autoscaling:CreateLaunchConfiguration\" - \"autoscaling:DeleteLaunchConfiguration\" - \"autoscaling:CreateAutoScalingGroup\" - \"autoscaling:DeleteAutoScalingGroup\" - \"autoscaling:TerminateInstanceInAutoScalingGroup\" - \"autoscaling:UpdateAutoScalingGroup\" - \"autoscaling:PutScalingPolicy\" - \"autoscaling:DeletePolicy\" - \"autoscaling:ExecutePolicy\" - \"autoscaling:DeleteLifecycleHook\" - \"autoscaling:PutLifecycleHook\" - \"autoscaling:BatchDeleteScheduledAction\" - \"autoscaling:DeleteScheduledAction\" - \"application-autoscaling:Describe*\" - \"application-autoscaling:*Tag*\" - \"application-autoscaling:RegisterScalableTarget\" - \"application-autoscaling:DeregisterScalableTarget\" - \"application-autoscaling:PutScalingPolicy\" - \"application-autoscaling:DeleteScalingPolicy\" - \"application-autoscaling:DeleteScheduledAction\" - \"application-autoscaling:PutScheduledAction\" Resource: \"*\" Roles: - !Ref Role ComputePolicy: Type: \"AWS::IAM::ManagedPolicy\" Properties: ManagedPolicyName: !Sub \"${AWS::StackName}-compute-policy\" PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \"ec2:Describe*\" - \"ec2:*Tag*\" - \"ec2:CreateImage\" - \"ec2:CopyImage\" - \"ec2:ExportImage\" - \"ec2:ImportImage\" - \"ec2:ImportImage\" - \"ec2:RegisterImage\" - \"ec2:DeregisterImage\" - \"ec2:StartInstances\" - \"ec2:RunInstances\" - \"ec2:RebootInstances\" - \"ec2:TerminateInstances\" - \"ec2:StopInstances\" - \"ec2:CreateInternetGateway\" - \"ec2:AttachInternetGateway\" - \"ec2:DetachInternetGateway\" - \"ec2:DeleteInternetGateway\" - \"ec2:CreateNatGateway\" - \"ec2:DeleteNatGateway\" - \"ec2:CreateRoute\" - \"ec2:RepalceRoute\" - \"ec2:DeleteRoute\" - \"ec2:CreateRouteTable\" - \"ec2:AssociateRouteTable\" - \"ec2:ReplaceRoute*\" - \"ec2:DisassociateRouteTable\" - \"ec2:DeleteRouteTable\" - \"ec2:CreateSecurityGroup\" - \"ec2:DeleteSecurityGroup\" - \"ec2:AuthorizeSecurityGroupIngress\" - \"ec2:RevokeSecurityGroupIngress\" - \"ec2:AuthorizeSecurityGroupEgress\" - \"ec2:RevokeSecurityGroupEgress\" - \"ec2:AssociateSubnetCidrBlock\" - \"ec2:CreateDefaultSubnet\" - \"ec2:CreateSubnet\" - \"ec2:CreateSubnetCidrReservation\" - \"ec2:DeleteSubnet\" - \"ec2:DeleteSubnetCidrReservation\" - \"ec2:DisassociateSubnetCidrBlock\" - \"ec2:GetSubnetCidrReservations\" - \"ec2:ModifySubnetAttribute\" - \"ec2:CreateVpc\" - \"ec2:DeleteVpc\" - \"ec2:ModifyVpcAttribute\" - \"ec2:AcceptTransitGatewayVpcAttachment\" - \"ec2:CreateTransitGatewayVpcAttachment\" - \"ec2:DeleteTransitGatewayVpcAttachment\" - \"ec2:ModifyTransitGatewayVpcAttachment\" - \"ec2:RejectTransitGatewayVpcAttachment\" - \"ec2:AcceptVpcPeeringConnection\" - \"ec2:CreateVpcPeeringConnection\" - \"ec2:DeleteVpcPeeringConnection\" - \"ec2:ModifyVpcPeeringConnectionOptions\" - \"ec2:RejectVpcPeeringConnection\" - \"ec2:AllocateAddress\" - \"ec2:ReleaseAddress\" - \"ec2:AssociateAddress\" - \"ec2:DisassociateAddress\" - \"ec2:ModifyAddressAttribute\" - \"ec2:ResetAddressAttribute\" - \"ec2:AllocateHosts\" - \"ec2:ModifyInstanceAttribute\" - \"ecr:Get*\" - \"ecr:*Tag*\" - \"ecr:CreateRepository\" - \"ecr:DescribeRepositories\" - \"ecr:DeleteRepository\" - \"ecr:PutImageScanningConfiguration\" - \"ecs:List*\" - \"ecs:Describe*\" - \"ecs:*Tag*\" - \"ecs:CreateCluster\" - \"ecs:UpdateCluster\" - \"ecs:DeleteCluster\" - \"ecs:CreateService\" - \"ecs:UpdateService\" - \"ecs:DeleteService\" - \"ecs:CreateTaskDefinition\" - \"ecs:UpdateTaskDefinition\" - \"ecs:DeleteTaskDefinition\" - \"ecs:DeregisterTaskDefinition\" - \"ecs:RegisterTaskDefinition\" - \"ecs:CreateCapacityProvider\" - \"ecs:DeleteCapacityProvider\" - \"ecs:DescribeCapacityProviders\" - \"ecs:PutClusterCapacityProviders\" - \"ecs:UpdateCapacityProvider\" Resource: \"*\" Roles: - !Ref Role CiCdPolicy: Type: \"AWS::IAM::ManagedPolicy\" Properties: ManagedPolicyName: !Sub \"${AWS::StackName}-ci-cd-policy\" PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \"codebuild:*Tag*\" - \"codebuild:List*\" - \"codebuild:BatchGet*\" - \"codebuild:CreateProject\" - \"codebuild:UpdateProject\" - \"codebuild:DeleteProject\" - \"codepipeline:List*\" - \"codepipeline:Get*\" - \"codepipeline:*Tag*\" - \"codepipeline:CreatePipeline\" - \"codepipeline:UpdatePipeline\" - \"codepipeline:DeletePipeline\" - \"codepipeline:StartPipelineExecution\" - \"codepipeline:StopPipelineExecution\" Resource: \"*\" Roles: - !Ref Role MonitoringPolicy: Type: \"AWS::IAM::ManagedPolicy\" Properties: ManagedPolicyName: !Sub \"${AWS::StackName}-monitoring-policy\" PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \"cloudtrail:*Tag*\" - \"cloudtrail:Describe*\" - \"cloudtrail:StartLogging\" - \"cloudtrail:PutEventSelectors\" - \"cloudtrail:*Trail\" - \"cloudtrail:ListTrails\" - \"cloudformation:*\" - \"cloudwatch:*Tag*\" - \"cloudwatch:DeleteDashboards\" - \"cloudwatch:Get*\" - \"cloudwatch:ListDashboards\" - \"cloudwatch:PutDashboard\" - \"logs:Get*\" - \"logs:Describe*\" - \"logs:*Tag*\" - \"logs:CreateLogDelivery\" - \"logs:ListLogDeliveries\" - \"logs:UpdateLogDelivery\" - \"logs:DeleteLogDelivery\" - \"logs:CreateLogGroup\" - \"logs:CreateLogStream\" - \"logs:PutResourcePolicy\" - \"logs:PutRetentionPolicy\" - \"logs:*LogEvents\" - \"ce:*\" Resource: \"*\" Roles: - !Ref Role EventStreamsPolicy: Type: \"AWS::IAM::ManagedPolicy\" Properties: ManagedPolicyName: !Sub \"${AWS::StackName}-event-streams-policy\" PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \"events:*Tag*\" - \"events:PutTargets\" - \"events:RemoveTargets\" - \"events:PutRule\" - \"events:DescribeRule\" - \"events:DeleteRule\" - \"sns:List*\" - \"sns:Get*\" - \"sns:*Tag*\" - \"sns:ConfirmSubscription\" - \"sns:CreateTopic\" - \"sns:DeleteTopic\" - \"sns:SetTopicAttributes\" - \"sns:ConfirmSubscription\" - \"sns:SetSubscriptionAttributes\" - \"sns:Subscribe\" - \"sns:Unsubscribe\" Resource: \"*\" Roles: - !Ref Role StoragePolicy: Type: \"AWS::IAM::ManagedPolicy\" Properties: ManagedPolicyName: !Sub \"${AWS::StackName}-storage-policy\" PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \"rds:Describe*\" - \"rds:*Tag*\" - \"rds:CreateDBSubnetGroup\" - \"rds:DeleteDBSubnetGroup\" - \"rds:CreateDBInstance\" - \"rds:ModifyDBInstance\" - \"rds:DeleteDBInstance\" - \"rds:CreateDBInstanceReadReplica\" - \"rds:PromoteReadReplica\" - \"s3:Describe*\" - \"s3:Get*\" - \"s3:List*\" - \"s3:*Tag*\" - \"s3:CreateBucket\" - \"s3:DeleteBucket\" - \"s3:PutBucketPolicy\" - \"s3:DeleteBucketPolicy\" - \"s3:PutEncryptionConfiguration\" - \"s3:PutBucketVersioning\" - \"s3:PutBucketPublicAccessBlock\" - \"s3:PutBucketAcl\" - \"s3:DeleteObject\" - \"s3:DeleteObjectVersion\" - \"s3:ObjectOwnerOverrideToBucketOwner\" - \"s3:PutBucketObjectLockConfiguration\" - \"s3:PutObject\" - \"s3:PutObjectAcl\" - \"s3:PutObjectLegalHold\" - \"s3:PutObjectRetention\" - \"s3:PutObjectVersionAcl\" - \"s3:ReplicateObject\" - \"s3:RestoreObject\" Resource: \"*\" Roles: - !Ref Role NetworkingPolicy: Type: \"AWS::IAM::ManagedPolicy\" Properties: ManagedPolicyName: !Sub \"${AWS::StackName}-networking-policy\" PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \"apigateway:*Tag*\" - \"apigateway:POST\" - \"apigateway:GET\" - \"apigateway:PUT\" - \"apigateway:PATCH\" - \"apigateway:DELETE\" - \"cloudfront:*Tag*\" - \"cloudfront:CreateDistribution\" - \"cloudfront:CreateDistributionWithTags\" - \"cloudfront:GetDistribution\" - \"elasticloadbalancing:Describe*\" - \"elasticloadbalancing:*Tag*\" - \"elasticloadbalancing:CreateListener\" - \"elasticloadbalancing:ModifyListener\" - \"elasticloadbalancing:DeleteListener\" - \"elasticloadbalancing:CreateLoadBalancer\" - \"elasticloadbalancing:DeleteLoadBalancer\" - \"elasticloadbalancing:CreateTargetGroup\" - \"elasticloadbalancing:ModifyTargetGroup\" - \"elasticloadbalancing:DeleteTargetGroup\" - \"elasticloadbalancing:ModifyTargetGroupAttributes\" - \"elasticloadbalancing:ModifyLoadBalancerAttributes\" - \"route53:Get*\" - \"route53:List*\" - \"route53:*Tag*\" - \"route53:AssociateVPCWithHostedZone\" - \"route53:CreateHostedZone\" - \"route53:DeleteHostedZone\" - \"route53:DisableHostedZoneDNSSEC\" - \"route53:DisassociateVPCFromHostedZone\" - \"route53:EnableHostedZoneDNSSEC\" - \"route53:UpdateHostedZoneComment\" - \"servicediscovery:Get*\" - \"servicediscovery:*Tag*\" - \"servicediscovery:CreateService\" - \"servicediscovery:DeleteService\" - \"servicediscovery:CreatePrivateDnsNamespace\" - \"servicediscovery:UpdatePrivateDnsNamespace\" - \"servicediscovery:DeleteNamespace\" - \"servicequotas:Get*\" Resource: \"*\" Roles: - !Ref Role ServerlessPolicy: Type: \"AWS::IAM::ManagedPolicy\" Properties: ManagedPolicyName: !Sub \"${AWS::StackName}-serverless-policy\" PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \"lambda:List*\" - \"lambda:Get*\" - \"lambda:*Tag*\" - \"lambda:CreateFunction\" - \"lambda:UpdateFunction\" - \"lambda:DeleteFunction\" - \"lambda:AddPermission\" - \"lambda:RemovePermission\" - \"lambda:InvokeFunction\" - \"lambda:DeleteFunctionCodeSigningConfig\" - \"lambda:DeleteFunctionConcurrency\" - \"lambda:DeleteFunctionEventInvokeConfig\" - \"lambda:PutFunctionCodeSigningConfig\" - \"lambda:PutFunctionConcurrency\" - \"lambda:PutFunctionEventInvokeConfig\" - \"lambda:UpdateFunctionCode\" - \"lambda:UpdateFunctionCodeSigningConfig\" - \"lambda:UpdateFunctionConfiguration\" - \"lambda:UpdateFunctionEventInvokeConfig\" - \"lambda:AddLayerVersionPermission\" - \"lambda:DeleteLayerVersion\" - \"lambda:PublishLayerVersion\" - \"lambda:RemoveLayerVersionPermission\" Resource: \"*\" Roles: - !Ref Role CloudTrailBucket: Type: AWS::S3::Bucket Properties: AccessControl: LogDeliveryWrite BucketName: !Sub \"${TinyStacksUserId}-${AWS::AccountId}\" Tags: - Key: TinyStacks_Owner Value: !Sub \"TinyStacks_${TinyStacksUserId}\" - Key: TinyStacks_StackName Value: !Ref AWS::StackName VersioningConfiguration: Status: Enabled UpdateReplacePolicy: Delete DeletionPolicy: Delete DependsOn: - IdentityAndSecretManagementPolicy - StoragePolicy CloudTrailBucketPolicy: Type: AWS::S3::BucketPolicy Properties: Bucket: !Ref CloudTrailBucket PolicyDocument: Version: 2012-10-17 Statement: - Action: s3:GetBucketAcl Effect: Allow Principal: Service: cloudtrail.amazonaws.com Resource: Fn::GetAtt: - CloudTrailBucket - Arn - Action: s3:PutObject Condition: StringEquals: s3:x-amz-acl: bucket-owner-full-control aws:SourceArn: !Sub - \"arn:aws:cloudtrail:${AWS::Region}:${AWS::AccountId}:trail/${CloudTrailName}\" - CloudTrailName: !Ref CloudTrail Effect: Allow Principal: Service: cloudtrail.amazonaws.com Resource: Fn::Join: - \"\" - - Fn::GetAtt: - CloudTrailBucket - Arn - !Sub \"/AWSLogs/${AWS::AccountId}/*\" CloudTrail: Type: AWS::CloudTrail::Trail Properties: IsLogging: true S3BucketName: !Ref CloudTrailBucket EnableLogFileValidation: true IncludeGlobalServiceEvents: true IsMultiRegionTrail: true Tags: - Key: TinyStacks_Owner Value: !Sub \"TinyStacks_${TinyStacksUserId}\" - Key: TinyStacks_StackName Value: !Ref AWS::StackName RegisterRoleWithTinyStacks: Type: \"Custom::notifyTS\" Properties: ServiceToken: arn:aws:sns:us-east-1:849087520365:tinystacks-register-role RoleArn: !GetAtt Role.Arn Region: !Ref \"AWS::Region\" TinyStacksUserToken: !Ref TinyStacksToken TinyStacksUserId: !Ref TinyStacksUserId DependsOn: - IdentityAndSecretManagementPolicy - Role - ScalingPolicy - ComputePolicy - CiCdPolicy - MonitoringPolicy - EventStreamsPolicy - StoragePolicy - NetworkingPolicy - ServerlessPolicy - CloudTrailBucket - CloudTrailBucketPolicy - CloudTrail","title":"Full AWS IAM Policy used by TinyStacks"},{"location":"load-balancers/","text":"To access your application, it needs to have a publicly available domain name, or endpoint . TinyStacks enables customers to choose one of two technologies for exposing the application endpoint: Amazon API Gateway , AWS's REST API management service, or Amazon Application Load Balancer (ALB), a more general-purpose load balancer suited to a large variety of Web applications. Both technologies help provide secure access to your application. As discussed in Networking , TinyStacks doesn't expose your application directly on the Internet. Instead, we use API Gateway or ALB to expose only the port required for your application. This gives the virtual machines running in your ECS cluster additional protection against attack and attempted intrusion. As its name suggests, ALB also provides load balancing services. This means that requests to your service are balanced across the different instances of your containers running on your ECS cluster. Standard Scale Applications and Hyperscale Applications The primary decision between choosing API Gateway versus ALB is how much traffic you expect your application to handle. We like to distinguish between two types of scale: Standard scale : Applications serving under one million requests a day. Hyperscale : Applications serving over one million requests. In general, standard scale applications will find better price and performance using API Gateway. Hyperscale applications) will receive better price/performance from using API Gateway. For more details, see our blog post comparing API Gateway with Application Load Balancer . Changing Your Load Balancer You can change the load balancer your application uses at any time. This can be useful if your application's usage has increased from a standard scale to a hyperscale pattern. To change your load balancer, go to Stack Settings (the gear icon next to your stack on the TinyStacks dashboard) and select Endpoint . From there, you can switch your endpoint technology between API Gateway and ALB. Enabling Caching on Endpoint Requests You can alternatively elect to enable caching on all endpoint requests. Enabling caching is best done for sites and services that act mostly as informational services or other scenarios where the return results are largely static. On API Gateway, caching is enabled directly as a feature on the API Gateway service. For Application Load Balancer, TinyStacks implements its own caching mechanism.","title":"Load balancers"},{"location":"load-balancers/#standard-scale-applications-and-hyperscale-applications","text":"The primary decision between choosing API Gateway versus ALB is how much traffic you expect your application to handle. We like to distinguish between two types of scale: Standard scale : Applications serving under one million requests a day. Hyperscale : Applications serving over one million requests. In general, standard scale applications will find better price and performance using API Gateway. Hyperscale applications) will receive better price/performance from using API Gateway. For more details, see our blog post comparing API Gateway with Application Load Balancer .","title":"Standard Scale Applications and Hyperscale Applications"},{"location":"load-balancers/#changing-your-load-balancer","text":"You can change the load balancer your application uses at any time. This can be useful if your application's usage has increased from a standard scale to a hyperscale pattern. To change your load balancer, go to Stack Settings (the gear icon next to your stack on the TinyStacks dashboard) and select Endpoint . From there, you can switch your endpoint technology between API Gateway and ALB.","title":"Changing Your Load Balancer"},{"location":"load-balancers/#enabling-caching-on-endpoint-requests","text":"You can alternatively elect to enable caching on all endpoint requests. Enabling caching is best done for sites and services that act mostly as informational services or other scenarios where the return results are largely static. On API Gateway, caching is enabled directly as a feature on the API Gateway service. For Application Load Balancer, TinyStacks implements its own caching mechanism.","title":"Enabling Caching on Endpoint Requests"},{"location":"logging-monitoring/","text":"TinyStacks maintains separate logs for each stage of your application's stack. TinyStacks maintains three logs viewable directly in the TinyStacks dashboard for your stack. Build Log The Build Log displays all output from your Docker container's build. If your container fails to build, you can find the error here and diagnose any issues. Runtime Logs Your Runtime Logs show any output from your running containers. Any application output written to standard output (e.g., console.log() in Node.js) from within your container will appear in this log. Access Logs Access Logs show who has been visiting your application. The format of the access logs will differ based on which endpoint technology you are using. An application that utilizes Amazon API Gateway will receive logs in API Gateway JSON format : 04/21/2022 18:27:42: { \"requestId\":\"Q8NnAiztIAMEV4w=\", \"ip\": \"73.239.102.103\", \"requestTime\":\"21/Apr/2022:17:09:26 +0000\", \"httpMethod\":\"GET\",\"routeKey\":\"$default\", \"status\":\"200\",\"protocol\":\"HTTP/1.1\", \"responseLength\":\"12\", \"resourcePath\":\"-\" } An application utilizing Application Load Balancer will receive logs in standard Apache Common Log Format: Accessing log data outside of TinyStacks Log data is stored in Amazon CloudWatch in your AWS account in various log groups. Your runtime logs as well as access logs are stored in log groups identified by tags. The tag takes the format: < stack-name >-< stack-name-and-stage >-< aws-account-number >-< stack-aws-region > So, for example, assume your AWS account ID is 111111111111 . If you have a stack named my-app with a stage named dev in the us-east-1 region, your stack's tag would be: my-app-my-app-dev-111111111111-us-east-1 . Using this, you can find CloudWatch Logs groups (and other TinyStacks-deployed assets) in your AWS accounts using tools such as the AWS Command Line Interface (CLI) . You can use these same tools to retrieve and export data from these logs and send them to your destination of choice. Monitoring Dashboard TinyStacks also supplies a monitoring dashboard as part of your stack's deployment. You can monitor multiple runtime metrics for your application, including request rate, latency, CPU utilization of your underlying ECS cluster instances, 400/500 errors, and more.","title":"Logging and monitoring"},{"location":"logging-monitoring/#build-log","text":"The Build Log displays all output from your Docker container's build. If your container fails to build, you can find the error here and diagnose any issues.","title":"Build Log"},{"location":"logging-monitoring/#runtime-logs","text":"Your Runtime Logs show any output from your running containers. Any application output written to standard output (e.g., console.log() in Node.js) from within your container will appear in this log.","title":"Runtime Logs"},{"location":"logging-monitoring/#access-logs","text":"Access Logs show who has been visiting your application. The format of the access logs will differ based on which endpoint technology you are using. An application that utilizes Amazon API Gateway will receive logs in API Gateway JSON format : 04/21/2022 18:27:42: { \"requestId\":\"Q8NnAiztIAMEV4w=\", \"ip\": \"73.239.102.103\", \"requestTime\":\"21/Apr/2022:17:09:26 +0000\", \"httpMethod\":\"GET\",\"routeKey\":\"$default\", \"status\":\"200\",\"protocol\":\"HTTP/1.1\", \"responseLength\":\"12\", \"resourcePath\":\"-\" } An application utilizing Application Load Balancer will receive logs in standard Apache Common Log Format:","title":"Access Logs"},{"location":"logging-monitoring/#accessing-log-data-outside-of-tinystacks","text":"Log data is stored in Amazon CloudWatch in your AWS account in various log groups. Your runtime logs as well as access logs are stored in log groups identified by tags. The tag takes the format: < stack-name >-< stack-name-and-stage >-< aws-account-number >-< stack-aws-region > So, for example, assume your AWS account ID is 111111111111 . If you have a stack named my-app with a stage named dev in the us-east-1 region, your stack's tag would be: my-app-my-app-dev-111111111111-us-east-1 . Using this, you can find CloudWatch Logs groups (and other TinyStacks-deployed assets) in your AWS accounts using tools such as the AWS Command Line Interface (CLI) . You can use these same tools to retrieve and export data from these logs and send them to your destination of choice.","title":"Accessing log data outside of TinyStacks"},{"location":"logging-monitoring/#monitoring-dashboard","text":"TinyStacks also supplies a monitoring dashboard as part of your stack's deployment. You can monitor multiple runtime metrics for your application, including request rate, latency, CPU utilization of your underlying ECS cluster instances, 400/500 errors, and more.","title":"Monitoring Dashboard"},{"location":"networking/","text":"Virtual Private Clouds (VPCs) One of the most technically complex aspects of setting up infrastructure on AWS is creating your Virtual Private Cloud, or VPC. A VPC establishes the IP address ranges available to your application in its own isolated address space, defines routing between your applications and other VPCs and the public Internet, and sets out security rules that control how others can interact with your network. A VPC defines a set of IP address ranges using logical groupings called subnets. Subnets can be one of three types: Public : Traffic is allowed in from and out to the Internet and between subnets within the VPC. Inbound traffic is still often restricted to specific ports (e.g., ports 80 and 443 for Web apps). Private : Traffic is allowed between subnets within the VPC. Requests can be made out to the public Internet via a NAT gateway. Isolated : Traffic is allowed between subnets within the VPC. There is no defined route for Internet requests. By using a combination of private/isolated and public subnets, application developers can create a secure hosting environment that reduces an application's surface attack area and minimizes the risk of intrusion. Designing and deploying a secure VPC usually requires a solid knowledge of networking and of AWS. TinyStacks eliminates this requirement by deploying every stack with a secure VPC by default. Each TinyStacks VPC consists of: Three public subnets . These subnets contain any public-facing infrastructure (e.g., your Application Load Balancer or API Gateway endpoints). Three isolated or private subnets . These subnets contain your ECS cluster instances and your database (if you provisioned it with TinyStacks). During stack creation process, we ask you if you want to add a NAT instance to your subnets. If you choose to add a NAT instance, the subnets are private. If you choose not to add a NAT instance, they are isolated.\u3000By default, we create a NAT instance and private subnets. Both sets of subnets are spread across different AWS Availability Zones . Critical resources are hosted across three separate subnets and Availability Zones. This helps protect against unexpected outages by placing the pieces of your infrastructure in geographically isolated data centers on AWS. For example, if you launch three Amazon ECS instances in your ECS cluster, we spread these evenly across all three availability zones This means your application can still keep serving traffic if a single availability zone goes down. How we connect your ECS cluster instances to the Internet depends on the scaling option you choose. If you use Application Load Balancer, the ALB can forward requests to your container instances by virtue of running in the same VPC. If you use API Gateway as your application front end, TinyStacks creates a VPC Link so that API Gateway can securely forward requests to your Docker container instances running on your cluster instances. For Security Groups Security groups in AWS define which resources on AWS or the public Internet can access certain AWS resources. We define security groups that restrict access to protected resources. For example, ECS cluster instances can only be accessed by your API Gateway or Application Load Balancer front ends at the specific port that you specify at stack creation time. Likewise, only the ECS cluster instances can talk to the database (unless you create a bastion host, as described below), and only on a single dedicated port. This architecture reduces your application's potential attack surface by only publicly exposing the minimal surface area required for your application to operate. Bastion Host By default, we do not allow any Internet connectivity directly to your database. However, your team may need to access the database outside of your application (e.g., to run ad hoc queries or troubleshoot issues). To facilitate this, TinyStacks supports creating a bastion host for your database. The bastion host is an Amazon EC2 instance that sits in one of your public subnets and to which you can connect via SSH using a cryptographic key. Once on the bastion host, you can connect directly to your database in its isolated subnets.","title":"Networking"},{"location":"networking/#virtual-private-clouds-vpcs","text":"One of the most technically complex aspects of setting up infrastructure on AWS is creating your Virtual Private Cloud, or VPC. A VPC establishes the IP address ranges available to your application in its own isolated address space, defines routing between your applications and other VPCs and the public Internet, and sets out security rules that control how others can interact with your network. A VPC defines a set of IP address ranges using logical groupings called subnets. Subnets can be one of three types: Public : Traffic is allowed in from and out to the Internet and between subnets within the VPC. Inbound traffic is still often restricted to specific ports (e.g., ports 80 and 443 for Web apps). Private : Traffic is allowed between subnets within the VPC. Requests can be made out to the public Internet via a NAT gateway. Isolated : Traffic is allowed between subnets within the VPC. There is no defined route for Internet requests. By using a combination of private/isolated and public subnets, application developers can create a secure hosting environment that reduces an application's surface attack area and minimizes the risk of intrusion. Designing and deploying a secure VPC usually requires a solid knowledge of networking and of AWS. TinyStacks eliminates this requirement by deploying every stack with a secure VPC by default. Each TinyStacks VPC consists of: Three public subnets . These subnets contain any public-facing infrastructure (e.g., your Application Load Balancer or API Gateway endpoints). Three isolated or private subnets . These subnets contain your ECS cluster instances and your database (if you provisioned it with TinyStacks). During stack creation process, we ask you if you want to add a NAT instance to your subnets. If you choose to add a NAT instance, the subnets are private. If you choose not to add a NAT instance, they are isolated.\u3000By default, we create a NAT instance and private subnets. Both sets of subnets are spread across different AWS Availability Zones . Critical resources are hosted across three separate subnets and Availability Zones. This helps protect against unexpected outages by placing the pieces of your infrastructure in geographically isolated data centers on AWS. For example, if you launch three Amazon ECS instances in your ECS cluster, we spread these evenly across all three availability zones This means your application can still keep serving traffic if a single availability zone goes down. How we connect your ECS cluster instances to the Internet depends on the scaling option you choose. If you use Application Load Balancer, the ALB can forward requests to your container instances by virtue of running in the same VPC. If you use API Gateway as your application front end, TinyStacks creates a VPC Link so that API Gateway can securely forward requests to your Docker container instances running on your cluster instances. For","title":"Virtual Private Clouds (VPCs)"},{"location":"networking/#security-groups","text":"Security groups in AWS define which resources on AWS or the public Internet can access certain AWS resources. We define security groups that restrict access to protected resources. For example, ECS cluster instances can only be accessed by your API Gateway or Application Load Balancer front ends at the specific port that you specify at stack creation time. Likewise, only the ECS cluster instances can talk to the database (unless you create a bastion host, as described below), and only on a single dedicated port. This architecture reduces your application's potential attack surface by only publicly exposing the minimal surface area required for your application to operate.","title":"Security Groups"},{"location":"networking/#bastion-host","text":"By default, we do not allow any Internet connectivity directly to your database. However, your team may need to access the database outside of your application (e.g., to run ad hoc queries or troubleshoot issues). To facilitate this, TinyStacks supports creating a bastion host for your database. The bastion host is an Amazon EC2 instance that sits in one of your public subnets and to which you can connect via SSH using a cryptographic key. Once on the bastion host, you can connect directly to your database in its isolated subnets.","title":"Bastion Host"},{"location":"security/","text":"At TinyStacks, we design all of our solutions with security in mind. In this article, we describe the measures that TinyStacks takes to ensure the security of your stack. We also discuss the permissions required by TinyStacks to operate on your connected accounts. Stack Security Every stack in TinyStacks is created with a secure architecture. The security measures we've taken include: Hosting your stack in a VPC with public/isolated subnets . TinyStacks hosts your stack with three public subnets and three isolated subnets. We place your (optional) PostgreSQL database and ECS cluster compute instances in the isolated subnets for additional protection. As discussed in our architecture overview , we use one of two methods to provide public access to your application, depending on the scaling option you're using: Application Load Balancer : Your ALB is hosted in your public subnets. Since it's located in the same VPC as your isolated subnets, it can interact freely with your containers over your container's exposed ports. API Gateway : If you use API Gateway, we create a VPC link between API Gateway and your isolated subnets. This allows for secure communication between your API endpoints and your containers without traffic flowing over the public Internet. Security groups . We define security groups on all components of your architecture that restrict communications to trusted entities. For example, our security group rules ensure that only your containers running in your isolated subnets can connect to the appropriate port on your TinyStacks-provisioned PostgreSQL database (and, optionally, your database bastion host if you create one). Similarly, if you use Application Load Balancer for your API endpoints, your containers restrict access to their exposed port(s) to the ALB instance running in your VPC. Secure database access . When TinyStacks creates a PostgreSQL database for your application, we create it inside your isolated subnets. This secures your database against access from outside your VPC. You can also optionally create a bastion host, which will enable secure SSH tunneling to your VPC in case your team requires direct database access. Encrypting secrets . All secrets used by your stack (such as your database connection information and credentials) are stored in encrypted format in AWS Secrets Manager . For example, when you ask TinyStacks to create a PostgreSQL database for your stack, TinyStacks stores the database name, hostname, username, password, and other sensitive access information in an entry in AWS Secrets Manager. TinyStacks does not store encryption keys or plaintext secrets for secrets hosted in customer accounts. Your team can access AWS Secrets Manager secrets securely through the AWS Console. Your application can also access these values using the AWS Command Line Interface (CLI) or the AWS SDK of your choice. Environment variables . If you have secrets you need to pass to your application (e.g., an access key/secret for AWS account access), you can pass them via your Docker containers using environment variables. This eliminates the need to hard-code security credentials into your code base or plain-text configuration files. S3 . S3 buckets are created for each region in a stack. These buckets are used to store access logs, build artifacts, and other supporting assets required to run stacks. S3 buckets are configured as private and they are versioned. AWS Permissions Required by TinyStacks TinyStacks makes calls to your AWS account for the purposes of creating or reclaiming stack infrastructure at your request. Also for maintenance, monitor, and report status back. In order to create a stack in your AWS account, TinyStacks first spins up an AWS CloudFormation template. This template takes three actions: Creates an AWS Identity and Access Management (IAM) role in your account. Gives the TinyStacks service role (tinystacks-service-role) the permission to assume this role. Informs TinyStacks (via an AWS Lambda callback) that the role is correctly configured for your account. The role that TinyStacks creates gives the TinyStacks service role the following access in your AWS account: CDK Role and Permissions There are a basic set of permissions created when your account is boostrapped by AWS's Cloud Development Kit. We grant our role the ability to assume the IAM role created through this process as well as full access to a CDK specific S3 Bucket, and read-only permission to CDK specific SSM Parameters. Amazon API Gateway Describe and tagging capabilities for all API Gateway resources Basic read/update/delete permissions Limited permissions for other operations Application Autoscaling Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations Autoscaling Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations Amazon CloudFront Tagging capabilities for all CloudFront resources Basic read/update/delete permissions Limited permissions for other operations AWS CloudFormation Full access for all CloudFormation capabilities Note that while this allows us to call a particular action in CloudFormation, the access to the specific action and resource is still controlled by the other permissions listed here. AWS CloudMap Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations AWS CloudTrail Describe and tagging capabilities for all resources Start Logging Limited permissions for other operations AWS CloudWatch Tagging capabilities for all CloudWatch resources Basic read/update/delete permissions Limited permissions for other operations AWS CloudWatch Logs Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations AWS CodeBuild Tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations AWS CodePipeline Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations AWS Cost Explorer Full access for all Cost Explorer capabilities Amazon Elastic Container Service (ECS) Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations Amazon Elastic Container Registry (ECR) Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations Amazon EC2 Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations Elastic Load Balancer Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations Amazon EventBridge Tagging capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations AWS Identity and Access Management Describe and tagging capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations Key Management Service (KMS) Tagging capabilities for all KMS resources Decrypt capability for KMS resources AWS Lambda Describe capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations Amazon Relational Database Service (RDS) Describe and tagging capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations Amazon Route 53 Describe and tagging capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations AWS Secrets Manager Describe capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations AWS Systems Manager Tagging capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations Amazon Simple Notification Service (SNS) Describe and tagging capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations Amazon Simple Storage Service (S3) Describe and tagging capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations The CloudFormation template deployment that TinyStacks creates begins with the name TinyStacksRole . If you wish to revoke TinyStacks' access to your AWS account, you can delete this deployment at any time. Note that, once deleted, TinyStacks will no longer be able to stand up, modify, or present the status of stacks in your AWS account unless you re-deploy it. You can see the full role in your AWS account's IAM dashboard. You can also view it on our IAM Policy page . Git Repository Permissions Required by TinyStacks In order to access your Git account, TinyStacks will request permissions for your Git repositories residing in GitHub or GitLab. GitHub permissions requested include: Read access to organizational metadata Administrator access to repositories. You can either allow access to all repositories or restrict GitHub to accessing select repositories only Read/write access to administration, checks, code, commit statuses, deployments, discussions, packages, pull requests, and webhooks. GitLab permissions requested include: Read access to organizational metadata Administrator access to projects Read/write access to administration (CRUD operations on repositories), checks, commit statuses, contents, deployments, discussions, packages, and pull requests. The exact permissions requested from GitHub and GitLab may change over time.","title":"How we secure your stack"},{"location":"security/#stack-security","text":"Every stack in TinyStacks is created with a secure architecture. The security measures we've taken include: Hosting your stack in a VPC with public/isolated subnets . TinyStacks hosts your stack with three public subnets and three isolated subnets. We place your (optional) PostgreSQL database and ECS cluster compute instances in the isolated subnets for additional protection. As discussed in our architecture overview , we use one of two methods to provide public access to your application, depending on the scaling option you're using: Application Load Balancer : Your ALB is hosted in your public subnets. Since it's located in the same VPC as your isolated subnets, it can interact freely with your containers over your container's exposed ports. API Gateway : If you use API Gateway, we create a VPC link between API Gateway and your isolated subnets. This allows for secure communication between your API endpoints and your containers without traffic flowing over the public Internet. Security groups . We define security groups on all components of your architecture that restrict communications to trusted entities. For example, our security group rules ensure that only your containers running in your isolated subnets can connect to the appropriate port on your TinyStacks-provisioned PostgreSQL database (and, optionally, your database bastion host if you create one). Similarly, if you use Application Load Balancer for your API endpoints, your containers restrict access to their exposed port(s) to the ALB instance running in your VPC. Secure database access . When TinyStacks creates a PostgreSQL database for your application, we create it inside your isolated subnets. This secures your database against access from outside your VPC. You can also optionally create a bastion host, which will enable secure SSH tunneling to your VPC in case your team requires direct database access. Encrypting secrets . All secrets used by your stack (such as your database connection information and credentials) are stored in encrypted format in AWS Secrets Manager . For example, when you ask TinyStacks to create a PostgreSQL database for your stack, TinyStacks stores the database name, hostname, username, password, and other sensitive access information in an entry in AWS Secrets Manager. TinyStacks does not store encryption keys or plaintext secrets for secrets hosted in customer accounts. Your team can access AWS Secrets Manager secrets securely through the AWS Console. Your application can also access these values using the AWS Command Line Interface (CLI) or the AWS SDK of your choice. Environment variables . If you have secrets you need to pass to your application (e.g., an access key/secret for AWS account access), you can pass them via your Docker containers using environment variables. This eliminates the need to hard-code security credentials into your code base or plain-text configuration files. S3 . S3 buckets are created for each region in a stack. These buckets are used to store access logs, build artifacts, and other supporting assets required to run stacks. S3 buckets are configured as private and they are versioned.","title":"Stack Security"},{"location":"security/#aws-permissions-required-by-tinystacks","text":"TinyStacks makes calls to your AWS account for the purposes of creating or reclaiming stack infrastructure at your request. Also for maintenance, monitor, and report status back. In order to create a stack in your AWS account, TinyStacks first spins up an AWS CloudFormation template. This template takes three actions: Creates an AWS Identity and Access Management (IAM) role in your account. Gives the TinyStacks service role (tinystacks-service-role) the permission to assume this role. Informs TinyStacks (via an AWS Lambda callback) that the role is correctly configured for your account. The role that TinyStacks creates gives the TinyStacks service role the following access in your AWS account: CDK Role and Permissions There are a basic set of permissions created when your account is boostrapped by AWS's Cloud Development Kit. We grant our role the ability to assume the IAM role created through this process as well as full access to a CDK specific S3 Bucket, and read-only permission to CDK specific SSM Parameters. Amazon API Gateway Describe and tagging capabilities for all API Gateway resources Basic read/update/delete permissions Limited permissions for other operations Application Autoscaling Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations Autoscaling Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations Amazon CloudFront Tagging capabilities for all CloudFront resources Basic read/update/delete permissions Limited permissions for other operations AWS CloudFormation Full access for all CloudFormation capabilities Note that while this allows us to call a particular action in CloudFormation, the access to the specific action and resource is still controlled by the other permissions listed here. AWS CloudMap Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations AWS CloudTrail Describe and tagging capabilities for all resources Start Logging Limited permissions for other operations AWS CloudWatch Tagging capabilities for all CloudWatch resources Basic read/update/delete permissions Limited permissions for other operations AWS CloudWatch Logs Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations AWS CodeBuild Tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations AWS CodePipeline Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations AWS Cost Explorer Full access for all Cost Explorer capabilities Amazon Elastic Container Service (ECS) Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations Amazon Elastic Container Registry (ECR) Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations Amazon EC2 Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations Elastic Load Balancer Describe and tagging capabilities for all resources Basic read/update/delete permissions Limited permissions for other operations Amazon EventBridge Tagging capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations AWS Identity and Access Management Describe and tagging capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations Key Management Service (KMS) Tagging capabilities for all KMS resources Decrypt capability for KMS resources AWS Lambda Describe capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations Amazon Relational Database Service (RDS) Describe and tagging capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations Amazon Route 53 Describe and tagging capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations AWS Secrets Manager Describe capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations AWS Systems Manager Tagging capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations Amazon Simple Notification Service (SNS) Describe and tagging capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations Amazon Simple Storage Service (S3) Describe and tagging capabilities for all resources Basic create/update/delete permissions Limited permissions for other operations The CloudFormation template deployment that TinyStacks creates begins with the name TinyStacksRole . If you wish to revoke TinyStacks' access to your AWS account, you can delete this deployment at any time. Note that, once deleted, TinyStacks will no longer be able to stand up, modify, or present the status of stacks in your AWS account unless you re-deploy it. You can see the full role in your AWS account's IAM dashboard. You can also view it on our IAM Policy page .","title":"AWS Permissions Required by TinyStacks"},{"location":"security/#git-repository-permissions-required-by-tinystacks","text":"In order to access your Git account, TinyStacks will request permissions for your Git repositories residing in GitHub or GitLab. GitHub permissions requested include: Read access to organizational metadata Administrator access to repositories. You can either allow access to all repositories or restrict GitHub to accessing select repositories only Read/write access to administration, checks, code, commit statuses, deployments, discussions, packages, pull requests, and webhooks. GitLab permissions requested include: Read access to organizational metadata Administrator access to projects Read/write access to administration (CRUD operations on repositories), checks, commit statuses, contents, deployments, discussions, packages, and pull requests. The exact permissions requested from GitHub and GitLab may change over time.","title":"Git Repository Permissions Required by TinyStacks"},{"location":"serverless/","text":"With TinyStacks, you have the option to run your code as a serverless application through AWS Lambda . Running serverless confers several benefits: It's cheaper for stacks or stages with low usage followed by large bursts of activity. With containers, an ECS cluster incurs a steady cost, as you are charged continuously for at least the minimum cluster capacity that you configured via TinyStacks. With Lambda, you're only charged for your code's actual execution time, which is measured on a per-millisecond basis. It's faster. The first access on a Lambda application can sometimes take a few seconds to load, as Lambda must download your code for the first time. After this initial access, however, Lambda applications execute subsequent requests very quickly. Deploying a serverless stack To deploy a serverless stack, create a new stack and, on the Review Infrastructure screen, select Serverless as your architecture near the top of the page. Note that there is no way to convert an existing stack between container and serverless. If you have an application running on container architecture, you will need to launch a new stack to convert it to running serverless. Configuring serverless architecture options Once you've selected Serverless, you can configure a few additional options. Networking By default, your serverless application will not run in a Virtual Private Cloud (VPC) . You can change this by clicking the gear in the Networking box on this page and selecting an existing VPC from your AWS account. Configure Lambda scale settings You can click the gear in the AWS Lambda box to select scaling options for your Lambda function. You can configure two options: Memory . The memory, in megabytes, allotted to your Lambda function. This should be large enough to run your containerized application. You can raise this to a maximum of 10,240 MB. Timeout . How long your function can run without returning output to its caller before the connection is severed and no data returned. You should raise this value only if your function has a long initial start-up time, or is executing a long-running task. The maximum value is 900 seconds (15 minutes).","title":"Serverless"},{"location":"serverless/#deploying-a-serverless-stack","text":"To deploy a serverless stack, create a new stack and, on the Review Infrastructure screen, select Serverless as your architecture near the top of the page. Note that there is no way to convert an existing stack between container and serverless. If you have an application running on container architecture, you will need to launch a new stack to convert it to running serverless.","title":"Deploying a serverless stack"},{"location":"serverless/#configuring-serverless-architecture-options","text":"Once you've selected Serverless, you can configure a few additional options.","title":"Configuring serverless architecture options"},{"location":"serverless/#networking","text":"By default, your serverless application will not run in a Virtual Private Cloud (VPC) . You can change this by clicking the gear in the Networking box on this page and selecting an existing VPC from your AWS account.","title":"Networking"},{"location":"serverless/#configure-lambda-scale-settings","text":"You can click the gear in the AWS Lambda box to select scaling options for your Lambda function. You can configure two options: Memory . The memory, in megabytes, allotted to your Lambda function. This should be large enough to run your containerized application. You can raise this to a maximum of 10,240 MB. Timeout . How long your function can run without returning output to its caller before the connection is severed and no data returned. You should raise this value only if your function has a long initial start-up time, or is executing a long-running task. The maximum value is 900 seconds (15 minutes).","title":"Configure Lambda scale settings"},{"location":"service-connections/","text":"You will need the following resources set up before you start using TinyStacks: A Git account with either GitHub or GitLab . An AWS account . All of the resources TinyStacks creates for you will be hosted in an AWS account that you own. Don't worry - you cna get started without any previous knowledge of AWS! Configure Git connection When you first log in to TinyStacks, you\u2019ll need to wire up a supported Git repository account as well as an AWS account. TinyStacks supports using Git repositories hosted on GitHub or GitLab. Select which Git service you plan to use: GitHub or Gitlab. A separate window will open. If you are not logged in to your selected service, you will be prompted to log in now. Once logged in, you will need to grant authorization for TinyStacks to access your GitHub or GitLab account. This will allow us to add a repository to your account (if you use one of our starter projects) or read an existing repository (if you use your own project). Configure AWS account Once you've authorized your Git account, you will be prompted to authorize an AWS account. This account will host all of the cloud resources required by your application. If you have not logged in to your AWS account recently, you will be prompted to do so. Once you have logged in to AWS, you will see the following screen, which prompts you to create an AWS CloudFormation stack in your AWS account. This step is necessary in order to proceed and fully connect your account. This CloudFormation template will create an AWS Identity and Access Management (IAM) role that TinyStacks will assume in order to create resources in your account. ( Note : Some elements of the screen above have been blurred out for security reasons.) To create this stack, select the box I acknowledge that AWS CloudFormation might create IAM resources . Then, click the Create Stack button. After creating the stack, tab back to the previous tab containing the TinyStacks window. Once the stack has completed creating, TinyStacks will automatically detect this and move you to the next step in the stack creation process. Once you see the screen below, you can proceed.","title":"Configure AWS and Git service connections"},{"location":"service-connections/#configure-git-connection","text":"When you first log in to TinyStacks, you\u2019ll need to wire up a supported Git repository account as well as an AWS account. TinyStacks supports using Git repositories hosted on GitHub or GitLab. Select which Git service you plan to use: GitHub or Gitlab. A separate window will open. If you are not logged in to your selected service, you will be prompted to log in now. Once logged in, you will need to grant authorization for TinyStacks to access your GitHub or GitLab account. This will allow us to add a repository to your account (if you use one of our starter projects) or read an existing repository (if you use your own project).","title":"Configure Git connection"},{"location":"service-connections/#configure-aws-account","text":"Once you've authorized your Git account, you will be prompted to authorize an AWS account. This account will host all of the cloud resources required by your application. If you have not logged in to your AWS account recently, you will be prompted to do so. Once you have logged in to AWS, you will see the following screen, which prompts you to create an AWS CloudFormation stack in your AWS account. This step is necessary in order to proceed and fully connect your account. This CloudFormation template will create an AWS Identity and Access Management (IAM) role that TinyStacks will assume in order to create resources in your account. ( Note : Some elements of the screen above have been blurred out for security reasons.) To create this stack, select the box I acknowledge that AWS CloudFormation might create IAM resources . Then, click the Create Stack button. After creating the stack, tab back to the previous tab containing the TinyStacks window. Once the stack has completed creating, TinyStacks will automatically detect this and move you to the next step in the stack creation process. Once you see the screen below, you can proceed.","title":"Configure AWS account"},{"location":"sidecars/","text":"In TinyStacks, you can run multiple Docker images in the same stack. These are commonly called sidecar images, because they run alongside your main container and share resources with it (networking, storage, etc.). Sidecars are useful for providing ancillary services to your main Docker image. For example, you can use a sidecar to offload application logging to a secondary image. To add sidecar Docker images, after you create your stack, navigate to the Stack settings page via the gear icon from the Stacks dashboard. Under Stack settings, click the dropdown next to Service: , and click Create new service . You can use two different methods to add a new service: Repository . Specify a Git repository. TinyStacks will look for the same files it requires when building a Docker image for your primary stack. Image URL . Use an existing Docker image URL. The repository containing the image must either be public or a repository in an AWS account to which you have given TinyStacks access . Alternatively, instead of a URL, you can specify the image name and version of an image in the Docker public repository(e.g., python:3.9 ). Once you're done, click Create new service . When the stack rebuilds, it will deploy your second container side by side on the same EC2 instances as your other running tasks. The images will scale together as you change your auto-scale settings. If you add an additional EC2 instance to your ECS cluster, we will create a new instance of each image on this new ECS cluster node. Memory and CPU resources on the instance are evenly split between all running container images. Sidecar Settings You can set basic settings for your sidecar by navigating to your stack's Stack settings page. Underneath Stack settings , you will see a service drop-down. Click this to select your sidecar service from the list. From here, you can configure all of the settings discussed in Basic Settings .","title":"Sidecars"},{"location":"sidecars/#sidecar-settings","text":"You can set basic settings for your sidecar by navigating to your stack's Stack settings page. Underneath Stack settings , you will see a service drop-down. Click this to select your sidecar service from the list. From here, you can configure all of the settings discussed in Basic Settings .","title":"Sidecar Settings"},{"location":"stages/","text":"As discussed in Concepts , stages are one of the fundamental building blocks of a TinyStacks release. A stage represents a deployment for a specific use case in the development process - such as development, testing, production release, etc. You can also use stages to manage multiple production deployments - e.g., for supporting multi-region deployments. By default, Stage ordering Stages are executed in the order shown in the TinyStacks dashboard. You can always add a new stage and insert it anywhere in the deployment order. It is not otherwise possible to change the position of a stage once it has been created; in that case, you will need to delete and re-create the stage. Stage Flow A check-in to a stack\u2019s Git repository will flow through each stage in the order shown on the TinyStacks Dashboard. Each stage will build and deploy the checked-in changes to that stage\u2019s stack. If the build and deployment are successful, the changes flow to the next stage, where they are also built and deployed. If an error occurs, the change will not flow to the next stage. This prevents changes that have failed to build or release in a given stage from propagating to later stages - e.g., your production environment. Stopping such failures earlier in the release process helps preserve the integrity and performance of your production application. Automatic and manual approvals By default, if a change checked into a branch is built and deployed successfully by one stage, it will propagate automatically to the next stage. In most cases, however, your team will want to test and approve changes manually before pushing them into certain critical stages, such as production environments. TinyStacks enables you to configure any stage so that it requires a manual approval before changes flow from the previous stage. Manual approvals are done via the TinyStacks dashboard. Adding a new stage When TinyStacks created your stack, we created a single stage for you called dev . As discussed in our concepts documentation , your team will likely want at least two stages - one for development and one for production. This allows you to test changes in your dev stage before releasing them to customers. To add a second stage, navigate to your Stacks page and select your stack by clicking on its name. The next page will show all of the stages currently defined for your stack. To add a new stage, click the Add stage button. On the New stack stage screen, you can configure your new stage. In the next screenshot, we've supplied a sample dialog for a prod stage that we explain in detail below: Stage name . Must be unique within this stack. Should indicate the role that this stage plays in your stack - e.g., dev, test, staging, prod, etc. In this case, we use the name prod to represent our production stage. Database . Enables adding a new Postgres database or another existing Amazon RDS database to your stack. For now, we'll leave this option blank. Require manual approval . Since we're creating a production stage here, we'll check this box. Amazon API Gateway vs. Application Load Balancer . Here, you can choose how you want to route requests to your application based on whether you expect your application will need to operate at standard scale or hyperscale. For now, we'll leave this to the default of Amazon API Gateway. For more information on scale settings, consult our architectural overview . Enable caching . Whether to cache responses through API Gateway. This option is only available if using Amazon API Gateway. Once you've configured your stage to suit your needs, click Create new stage . The new stage will run and create, building the code that you most recently checked in to the stack's associated Git branch. Deleting a stage You can delete a stage from the stage's settings page. To delete the stage, select Stack Details and click the Delete stack button. When prompted, confirm the deletion by typing the word delete . Switching a stage from automatic to manual approvals Sometimes, you'll want to move a stage from an automatic to manual approval. To do this, navigate to Stack settings . Underneath Stage settings , choose the stage you want to modify from the drop-down and then choose Stage details . Here you can mark the checkbox Requires manual approval to turn on manual approvals for the currently selected stage. New stages pending and manual approvals It's possible to insert a new stage before a stage that has a pending manual approval. In this case, the new stage will deploy the changes from its previous stage immediately. If you approve the release to the next stage, the deployment will occur immediately, even if the new stage is still in the middle of its deployment. Configuring multi-region deployments Many projects need to deploy their application to multiple regions for various reasons: To support higher availability from clients in different locations around the world To deploy into a \"canary\" or test region To enable a backup or disaster recovery version of your application You can easily support multi-region deployments by creating a new stage that targets a different AWS region. For example, let's say your existing production release targets AWS' us-east-1 region. You can create a second production deployment after that region that deploys the same infrastructure into a second region, such as us-west-2. Canary region One common pattern for multi-region deployments is for your initial production deployment to occur in a canary region . This is a \"test\" region that behaves exactly like production but is only used by your team (and, potentially, stakeholders or early adopters) to validate that a new release installs correctly and is performing in line with expectations. On TinyStacks, you can set up a canary region deployment easily. Assuming your application is primarily hosted in us-east-1 and that you wanted to set up a canary in us-west-2, you would: Enable manual approvals on the production stage Create a new stage before the production stage (called something like canary ) that installs into us-west-2 Copy any build and runtime environment variables that your production stage uses into the new stage Wait for the deployment to finish and then test your application in us-west-2 If the tests succeed, manually approve promotion of your changes to the production stage Keep in mind that standing up another copy of your stack will incur additional costs for every minute it is running. Currently, y ou can control costs in such deployments by reducing the amount of running infrastructure - e.g., by setting the number of desired instances in your ECS cluster to zero.","title":"Stages"},{"location":"stages/#stage-ordering","text":"Stages are executed in the order shown in the TinyStacks dashboard. You can always add a new stage and insert it anywhere in the deployment order. It is not otherwise possible to change the position of a stage once it has been created; in that case, you will need to delete and re-create the stage.","title":"Stage ordering"},{"location":"stages/#stage-flow","text":"A check-in to a stack\u2019s Git repository will flow through each stage in the order shown on the TinyStacks Dashboard. Each stage will build and deploy the checked-in changes to that stage\u2019s stack. If the build and deployment are successful, the changes flow to the next stage, where they are also built and deployed. If an error occurs, the change will not flow to the next stage. This prevents changes that have failed to build or release in a given stage from propagating to later stages - e.g., your production environment. Stopping such failures earlier in the release process helps preserve the integrity and performance of your production application.","title":"Stage Flow"},{"location":"stages/#automatic-and-manual-approvals","text":"By default, if a change checked into a branch is built and deployed successfully by one stage, it will propagate automatically to the next stage. In most cases, however, your team will want to test and approve changes manually before pushing them into certain critical stages, such as production environments. TinyStacks enables you to configure any stage so that it requires a manual approval before changes flow from the previous stage. Manual approvals are done via the TinyStacks dashboard.","title":"Automatic and manual approvals"},{"location":"stages/#adding-a-new-stage","text":"When TinyStacks created your stack, we created a single stage for you called dev . As discussed in our concepts documentation , your team will likely want at least two stages - one for development and one for production. This allows you to test changes in your dev stage before releasing them to customers. To add a second stage, navigate to your Stacks page and select your stack by clicking on its name. The next page will show all of the stages currently defined for your stack. To add a new stage, click the Add stage button. On the New stack stage screen, you can configure your new stage. In the next screenshot, we've supplied a sample dialog for a prod stage that we explain in detail below: Stage name . Must be unique within this stack. Should indicate the role that this stage plays in your stack - e.g., dev, test, staging, prod, etc. In this case, we use the name prod to represent our production stage. Database . Enables adding a new Postgres database or another existing Amazon RDS database to your stack. For now, we'll leave this option blank. Require manual approval . Since we're creating a production stage here, we'll check this box. Amazon API Gateway vs. Application Load Balancer . Here, you can choose how you want to route requests to your application based on whether you expect your application will need to operate at standard scale or hyperscale. For now, we'll leave this to the default of Amazon API Gateway. For more information on scale settings, consult our architectural overview . Enable caching . Whether to cache responses through API Gateway. This option is only available if using Amazon API Gateway. Once you've configured your stage to suit your needs, click Create new stage . The new stage will run and create, building the code that you most recently checked in to the stack's associated Git branch.","title":"Adding a new stage"},{"location":"stages/#deleting-a-stage","text":"You can delete a stage from the stage's settings page. To delete the stage, select Stack Details and click the Delete stack button. When prompted, confirm the deletion by typing the word delete .","title":"Deleting a stage"},{"location":"stages/#switching-a-stage-from-automatic-to-manual-approvals","text":"Sometimes, you'll want to move a stage from an automatic to manual approval. To do this, navigate to Stack settings . Underneath Stage settings , choose the stage you want to modify from the drop-down and then choose Stage details . Here you can mark the checkbox Requires manual approval to turn on manual approvals for the currently selected stage.","title":"Switching a stage from automatic to manual approvals"},{"location":"stages/#new-stages-pending-and-manual-approvals","text":"It's possible to insert a new stage before a stage that has a pending manual approval. In this case, the new stage will deploy the changes from its previous stage immediately. If you approve the release to the next stage, the deployment will occur immediately, even if the new stage is still in the middle of its deployment.","title":"New stages pending and manual approvals"},{"location":"stages/#configuring-multi-region-deployments","text":"Many projects need to deploy their application to multiple regions for various reasons: To support higher availability from clients in different locations around the world To deploy into a \"canary\" or test region To enable a backup or disaster recovery version of your application You can easily support multi-region deployments by creating a new stage that targets a different AWS region. For example, let's say your existing production release targets AWS' us-east-1 region. You can create a second production deployment after that region that deploys the same infrastructure into a second region, such as us-west-2.","title":"Configuring multi-region deployments"},{"location":"stages/#canary-region","text":"One common pattern for multi-region deployments is for your initial production deployment to occur in a canary region . This is a \"test\" region that behaves exactly like production but is only used by your team (and, potentially, stakeholders or early adopters) to validate that a new release installs correctly and is performing in line with expectations. On TinyStacks, you can set up a canary region deployment easily. Assuming your application is primarily hosted in us-east-1 and that you wanted to set up a canary in us-west-2, you would: Enable manual approvals on the production stage Create a new stage before the production stage (called something like canary ) that installs into us-west-2 Copy any build and runtime environment variables that your production stage uses into the new stage Wait for the deployment to finish and then test your application in us-west-2 If the tests succeed, manually approve promotion of your changes to the production stage Keep in mind that standing up another copy of your stack will incur additional costs for every minute it is running. Currently, y ou can control costs in such deployments by reducing the amount of running infrastructure - e.g., by setting the number of desired instances in your ECS cluster to zero.","title":"Canary region"},{"location":"troubleshooting/","text":"When using TinyStacks, you may run into the following issues. Most customers have resolved these issues with the tips below. If a given resolution doesn't work for you, please contact us on our Discord channel and we'll help you out. Launch is stuck for several minutes On occasion, you may see a stack launched stuck in one of its status states for several minutes. In these situations, it's possible that the stack has encountered an error that's preventing it from launching. Here are a few things you can check or try: Check for CloudFormation errors Sign in to the AWS Management Console and check the CloudFormation template's Events tab for any errors. (The template name will begin with the name of your stack.) It's possible the stack encountered an error in creating the Amazon ECS cluster, in which case the detailed error message should provide actionable information. Drop the service's desired task count in ECS If you're just standing up your initial infrastructure, there may be an issue with the application on your Docker container that's preventing your service from starting on ECS. Dropping your task count down to zero in the Amazon ECS Management Console can help skip over any such errors and ensure your stack finishes creating correctly. Find the ECS cluster with the same name as your stack and click on it. From there, click on the name of your service under Services . Click the Edit button and set the field Desired tasks to 0. Turn off the application's health check If your application is failing its health check, this will stop the deployment from registering as completed. To do this, remove any custom health check setting. Navigate to Settings -> Stack Settings and then, under your service, select Settings to view and change your health check URL. Error When Enabling Logging: \"Policy document length breaking CloudWatch Log Constraints\" When TinyStacks attempts to enable logging on your stack, you may see the following error in the console: Cannot enable logging. Policy document length breaking Cloudwatch Logs Constraints, either < 1 or > 5120 (Service: AmazonApiGatewayV2; Status Code: 400; Error Code: BadRequestException; To resolve this, run the following AWS CLI command. You can run this command on the AWS CloudShell , which has the CLI pre-installed and pre-authenticates you directly from the AWS Console. (Alternative, you can run this on your local computer if you have the AWS CLI installed and configured for use with your AWS account.) aws logs put-resource-policy --policy-name AWSLogDeliveryWrite20150319 --policy-document \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Sid\\\":\\\"AWSLogDeliveryWrite\\\",\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":{\\\"Service\\\":\\\"delivery.logs.amazonaws.com\\\"},\\\"Action\\\":[\\\"logs:CreateLogStream\\\",\\\"logs:PutLogEvents\\\"],\\\"Resource\\\":[\\\"*\\\"]}]}\" Error When Downloading Public Containers from Docker: \"You have reached your rate pull limit\" When building your stack, the build may fail with the following message in the Build Log: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading. The problem occurs when downloading an image from the Docker public registry. The issue is that Docker treats all requests coming from AWS in a specific region as coming from a single user. If too many other AWS users are pulling Docker images at the same time that your build is running, you could encounter this error. We have taken steps to limit the occurrence of this error, such as ensuring all Docker pull requests come from their own VPC. However, you may still see this error occur on occasion. To prevent it, you can edit your Dockerfile to switch out the base image used by your application and use an equivalent base image from the Amazon ECR Public Gallery , which is not subject to the same request limitations. You will need to edit the FROM line of your Dockerfile to change how the image is pulled. Below are links to equivalent images in the ECR Public Gallery for the sample applications published by TinyStacks: Python : Use the Bitnami Python image by editing your FROM line as follows: FROM public.ecr.aws/bitnami/python:latest Node.js/Express : Use the Bitnami Node.js image by editing your FROM line as follows: FROM public.ecr.aws/bitnami/node:latest We have modified the TinyStacks sample Docker applications to use these images from the ECR Public Gallery. Cannot see certain repositories in TinyStacks If someone in your organization configured a GitHub connection to only allow access to certain repositories, you may not see a repository that you'd like to launch with TinyStacks. To change which repositories TinyStacks can access, go to github.com and, on the right hand menu under your personal icon, select Settings -> Applications . From there, click Configure next to TinyStacks . From there, you can either allow all repositories or select additional repositories to make available to TinyStacks. Can't connect to bastion host Occasionally, your bastion host needs to be re-provisioned or swapped out for another instance. This can cause your bastion endpoint to change.","title":"Troubleshooting guide"},{"location":"troubleshooting/#launch-is-stuck-for-several-minutes","text":"On occasion, you may see a stack launched stuck in one of its status states for several minutes. In these situations, it's possible that the stack has encountered an error that's preventing it from launching. Here are a few things you can check or try:","title":"Launch is stuck for several minutes"},{"location":"troubleshooting/#check-for-cloudformation-errors","text":"Sign in to the AWS Management Console and check the CloudFormation template's Events tab for any errors. (The template name will begin with the name of your stack.) It's possible the stack encountered an error in creating the Amazon ECS cluster, in which case the detailed error message should provide actionable information.","title":"Check for CloudFormation errors"},{"location":"troubleshooting/#drop-the-services-desired-task-count-in-ecs","text":"If you're just standing up your initial infrastructure, there may be an issue with the application on your Docker container that's preventing your service from starting on ECS. Dropping your task count down to zero in the Amazon ECS Management Console can help skip over any such errors and ensure your stack finishes creating correctly. Find the ECS cluster with the same name as your stack and click on it. From there, click on the name of your service under Services . Click the Edit button and set the field Desired tasks to 0.","title":"Drop the service's desired task count in ECS"},{"location":"troubleshooting/#turn-off-the-applications-health-check","text":"If your application is failing its health check, this will stop the deployment from registering as completed. To do this, remove any custom health check setting. Navigate to Settings -> Stack Settings and then, under your service, select Settings to view and change your health check URL.","title":"Turn off the application's health check"},{"location":"troubleshooting/#error-when-enabling-logging-policy-document-length-breaking-cloudwatch-log-constraints","text":"When TinyStacks attempts to enable logging on your stack, you may see the following error in the console: Cannot enable logging. Policy document length breaking Cloudwatch Logs Constraints, either < 1 or > 5120 (Service: AmazonApiGatewayV2; Status Code: 400; Error Code: BadRequestException; To resolve this, run the following AWS CLI command. You can run this command on the AWS CloudShell , which has the CLI pre-installed and pre-authenticates you directly from the AWS Console. (Alternative, you can run this on your local computer if you have the AWS CLI installed and configured for use with your AWS account.) aws logs put-resource-policy --policy-name AWSLogDeliveryWrite20150319 --policy-document \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Sid\\\":\\\"AWSLogDeliveryWrite\\\",\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":{\\\"Service\\\":\\\"delivery.logs.amazonaws.com\\\"},\\\"Action\\\":[\\\"logs:CreateLogStream\\\",\\\"logs:PutLogEvents\\\"],\\\"Resource\\\":[\\\"*\\\"]}]}\"","title":"Error When Enabling Logging: \"Policy document length breaking CloudWatch Log Constraints\""},{"location":"troubleshooting/#error-when-downloading-public-containers-from-docker-you-have-reached-your-rate-pull-limit","text":"When building your stack, the build may fail with the following message in the Build Log: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading. The problem occurs when downloading an image from the Docker public registry. The issue is that Docker treats all requests coming from AWS in a specific region as coming from a single user. If too many other AWS users are pulling Docker images at the same time that your build is running, you could encounter this error. We have taken steps to limit the occurrence of this error, such as ensuring all Docker pull requests come from their own VPC. However, you may still see this error occur on occasion. To prevent it, you can edit your Dockerfile to switch out the base image used by your application and use an equivalent base image from the Amazon ECR Public Gallery , which is not subject to the same request limitations. You will need to edit the FROM line of your Dockerfile to change how the image is pulled. Below are links to equivalent images in the ECR Public Gallery for the sample applications published by TinyStacks: Python : Use the Bitnami Python image by editing your FROM line as follows: FROM public.ecr.aws/bitnami/python:latest Node.js/Express : Use the Bitnami Node.js image by editing your FROM line as follows: FROM public.ecr.aws/bitnami/node:latest We have modified the TinyStacks sample Docker applications to use these images from the ECR Public Gallery.","title":"Error When Downloading Public Containers from Docker: \"You have reached your rate pull limit\""},{"location":"troubleshooting/#cannot-see-certain-repositories-in-tinystacks","text":"If someone in your organization configured a GitHub connection to only allow access to certain repositories, you may not see a repository that you'd like to launch with TinyStacks. To change which repositories TinyStacks can access, go to github.com and, on the right hand menu under your personal icon, select Settings -> Applications . From there, click Configure next to TinyStacks . From there, you can either allow all repositories or select additional repositories to make available to TinyStacks.","title":"Cannot see certain repositories in TinyStacks"},{"location":"troubleshooting/#cant-connect-to-bastion-host","text":"Occasionally, your bastion host needs to be re-provisioned or swapped out for another instance. This can cause your bastion endpoint to change.","title":"Can't connect to bastion host"}]}